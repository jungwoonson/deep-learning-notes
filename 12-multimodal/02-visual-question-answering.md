# Visual Question Answering (VQA)

## VLA 연결고리

VQA는 이미지를 보고 질문에 답하는 task이다. VLA는 이 개념을 로봇으로 확장한 것이다. VQA가 "이 이미지에서 고양이는 몇 마리인가?"에 답한다면, VLA는 "이 장면에서 빨간 컵을 집으려면 **어떤 행동(action)을 해야 하는가?**"에 답한다. task의 구조가 동일하고, 출력만 다르다.

---

## 핵심 개념

### 1. VQA Task 정의

VQA는 다음을 입력으로 받아 답변을 생성하는 task이다:

```
입력: 이미지 + 자연어 질문
출력: 자연어 답변
```

예시:

- 이미지: 공원 사진 / 질문: "벤치에 몇 명이 앉아 있나?" → 답: "3명"
- 이미지: 주방 사진 / 질문: "오븐은 켜져 있나?" → 답: "아니오"

VQA는 단순한 이미지 분류를 넘어, **이미지 이해 + 언어 이해 + 추론**이 모두 필요하다.

### 2. 초기 접근 방식

**초기 VQA 모델의 구조:**

```
이미지 → CNN → 이미지 feature
질문   → RNN/LSTM → 질문 feature
        ↓
두 feature를 결합 (concatenate, element-wise multiply 등)
        ↓
분류기(classifier) → 미리 정해진 답변 후보 중 선택
```

**한계점:**

- 답변이 미리 정해진 후보 중 하나여야 함 (open-ended 답변 불가)
- 이미지와 질문의 상호작용이 제한적
- 복잡한 추론 (다단계 reasoning)이 어려움

### 3. Attention 기반 접근

이후 **attention mechanism**이 도입되어 개선되었다:

- 질문에 따라 이미지의 **관련 영역에 집중** (spatial attention)
- "고양이는 어디에 있나?" → 이미지에서 고양이 영역에 높은 attention
- 질문과 이미지 간의 상호작용이 더 풍부해짐

하지만 여전히 고정된 답변 후보에서 선택하는 방식이 주류였다.

### 4. LLM 기반 현대적 접근

대규모 언어 모델(LLM)의 등장으로 VQA가 근본적으로 변화했다:

```
이미지 → Vision Encoder → visual token sequence
질문   → Tokenizer → text token sequence
        ↓
  [visual tokens] + [text tokens] → LLM → 자유 형식 답변 생성
```

**핵심 변화:**

- 고정 후보가 아닌 **자유 형식(free-form) 텍스트 생성**
- LLM의 풍부한 세계 지식과 추론 능력 활용
- 더 복잡한 질문과 답변 처리 가능

대표 모델: LLaVA, GPT-4V, Gemini 등

### 5. VQA에서 VLA로의 전환

VQA와 VLA의 구조적 유사성:

| | VQA | VLA |
|---|-----|-----|
| 입력 | 이미지 + 질문 | 이미지 + 언어 명령 |
| 처리 | Vision Encoder + LLM | Vision Encoder + LLM |
| 출력 | 텍스트 답변 | **로봇 행동(action)** |
| 예시 | "컵이 어디 있나?" → "테이블 위" | "컵을 집어라" → [x, y, z, rx, ry, rz, grip] |

VLA는 본질적으로 **"행동을 출력하는 VQA"**이다. 이미지를 이해하고, 언어 명령을 해석하고, 적절한 응답(행동)을 생성한다. 이 구조적 동일성 덕분에, VQA/VLM의 발전이 곧바로 VLA의 발전으로 이어진다.

---

## 연습 주제

1. VQA에서 "이미지 이해", "언어 이해", "추론"이 각각 어떤 역할을 하는지 구체적 예시로 설명해 보라
2. 초기 VQA(분류 방식)와 LLM 기반 VQA(생성 방식)의 장단점을 비교해 보라
3. VQA의 출력을 "텍스트 답변"에서 "로봇 행동"으로 바꾸려면 구조적으로 무엇이 달라져야 하는지 생각해 보라
4. "빨간 컵을 테이블 왼쪽으로 옮겨라"라는 명령을 VLA가 처리하려면 어떤 이해 능력이 필요한지 나열해 보라
5. VQA에서 attention이 중요한 이유를 로봇 조작(manipulation) 관점에서 설명해 보라

---
