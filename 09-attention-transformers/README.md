# 어텐션 & 트랜스포머

현대 딥러닝의 핵심 아키텍처인 Transformer를 다룬다. Attention 메커니즘부터 Self-Attention, Multi-Head Attention, 위치 인코딩까지 단계적으로 분해하고 직접 구현한다.

| # | 주제 | 상태 |
|---|------|------|
| 1 | [어텐션 메커니즘](01-attention-mechanism.md) | |
| 2 | [Self-Attention (Q/K/V)](02-self-attention-qkv.md) | |
| 3 | [Transformer 아키텍처](03-transformer-architecture.md) | |
| 4 | [위치 인코딩 (RoPE)](04-positional-encoding-rope.md) | |
| 5 | [Multi-Head Attention & FFN](05-multi-head-attention-ffn.md) | |
| 6 | [Transformer 구현](06-transformer-implementation.md) | |
