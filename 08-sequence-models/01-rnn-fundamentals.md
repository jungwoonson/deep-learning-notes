# RNN 기초 (Recurrent Neural Network Fundamentals)

## VLA 연결고리

VLA(Vision-Language-Action)는 로봇이 시간 순서대로 행동을 생성하는 모델이다. "컵을 잡아라"라는 명령을 받으면 팔을 뻗고, 손을 벌리고, 잡고, 들어올리는 **순서가 있는 동작(sequence)**을 만들어야 한다. RNN은 이런 순차 데이터를 다루는 가장 기본적인 구조로, 이후 등장하는 LSTM, Seq2Seq, Transformer, 그리고 최종적으로 VLA까지 이어지는 출발점이다.

---

## 핵심 개념

### 1. Sequence Data (순차 데이터)

데이터의 **순서 자체가 의미**를 가지는 데이터를 말한다.

- 자연어: "나는 밥을 먹는다" - 단어 순서가 바뀌면 의미가 달라진다
- 시계열: 주가, 온도 등 시간에 따라 변하는 값
- 로봇 동작: 관절 각도가 시간에 따라 변하는 궤적(trajectory)

일반 신경망(feedforward network)은 입력을 독립적으로 처리하므로, 이전 입력의 정보를 기억하지 못한다. 순차 데이터를 다루려면 **과거 정보를 현재에 전달하는 구조**가 필요하다.

### 2. Vanilla RNN

RNN의 핵심 아이디어는 단순하다: **이전 시점의 출력을 현재 시점의 입력과 함께 사용한다.**

$$h_t = \tanh(W_{hh} \cdot h_{t-1} + W_{xh} \cdot x_t + b)$$

- $x_t$: 현재 시점 t의 입력
- $h_{t-1}$: 이전 시점의 hidden state
- $h_t$: 현재 시점의 hidden state (다음 시점으로 전달됨)
- $W_{hh}$, $W_{xh}$: 학습되는 가중치 (모든 시점에서 동일한 가중치를 공유)

같은 가중치를 매 시점마다 반복 적용하기 때문에 "Recurrent(순환)"라는 이름이 붙었다.

### 3. Hidden State (은닉 상태)

Hidden state는 RNN의 **메모리** 역할을 한다. 지금까지 본 입력들의 정보를 압축하여 하나의 벡터에 담고 있다.

- 시점 1: $h_1$은 $x_1$의 정보를 담는다
- 시점 2: $h_2$는 $x_1$과 $x_2$의 정보를 담는다
- 시점 t: $h_t$는 $x_1$부터 $x_t$까지의 정보를 담는다

이론적으로는 모든 과거 정보를 기억할 수 있지만, 실제로는 **최근 정보에 치우치는 한계**가 있다. 이 문제는 뒤에서 다룬다.

### 4. BPTT (Backpropagation Through Time)

RNN을 학습시키려면 시간 축을 따라 역전파를 수행해야 한다. 이것을 **BPTT**라고 한다.

RNN을 시간 축으로 펼치면(unfold), 각 시점이 하나의 레이어처럼 보인다. 10개의 단어를 처리하면 10층짜리 네트워크를 역전파하는 것과 비슷하다. 이때 모든 시점이 같은 가중치를 공유하므로, gradient는 각 시점의 기여분을 모두 합산한다.

### 5. Vanishing / Exploding Gradients

BPTT의 치명적 문제: 시간이 길어질수록 gradient가 **소멸(vanishing)**하거나 **폭발(exploding)**한다.

**Vanishing gradient**: gradient가 매 시점 곱해지면서 점점 0에 가까워진다. 결과적으로 먼 과거의 입력이 학습에 거의 영향을 미치지 못한다. 예를 들어 "나는 한국에서 태어나서 ... (100단어) ... **한국어**를 잘한다"에서 "한국에서"와 "한국어"의 관계를 학습하기 어렵다.

**Exploding gradient**: gradient가 매 시점 곱해지면서 극단적으로 커진다. 가중치가 한 번에 크게 변하면서 학습이 불안정해진다. 이 문제는 gradient clipping(일정 크기 이상이면 잘라내기)으로 비교적 쉽게 완화할 수 있다.

Vanishing gradient는 단순한 트릭으로 해결하기 어려우며, 이를 구조적으로 해결한 것이 **LSTM**과 **GRU**이다.

---

## 연습 주제

1. 길이가 5인 문장과 길이가 50인 문장을 각각 RNN에 넣었을 때, 첫 번째 단어의 정보가 마지막 hidden state에 얼마나 남아있을지 생각해 보기
2. Hidden state의 차원(dimension)이 커지면 어떤 장단점이 있는지 정리하기
3. RNN의 가중치 공유(weight sharing)가 왜 필요한지, 만약 각 시점마다 다른 가중치를 쓰면 어떤 문제가 생기는지 서술하기
4. Vanishing gradient와 exploding gradient 중 어떤 것이 더 해결하기 어려운지, 그 이유와 함께 정리하기
5. 로봇이 "물건을 집어서 옮기는" 동작 시퀀스에서 hidden state가 어떤 정보를 담아야 할지 자유롭게 상상해 보기

---
