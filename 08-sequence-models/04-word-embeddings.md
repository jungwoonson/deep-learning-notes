# 워드 임베딩 (Word Embeddings)

## VLA 연결고리

VLA에서 "빨간 컵을 집어"라는 텍스트 지시가 모델에 들어갈 때, 글자나 단어가 그대로 들어가는 것이 아니다. 각 토큰이 **고차원 벡터(embedding)**로 변환되어 입력된다. 이 임베딩이 LLM의 첫 번째 레이어이며, 단어의 의미를 수치로 표현하는 방법이다. 임베딩의 품질이 모델 전체의 언어 이해 능력을 좌우하므로, VLA가 지시를 정확히 이해하려면 좋은 임베딩이 필수이다.

---

## 핵심 개념

### 1. One-Hot Encoding의 한계

가장 단순한 단어 표현: 사전 크기만큼의 벡터에서 해당 단어 위치만 1, 나머지는 0으로 표시한다.

```
사전: [고양이, 강아지, 물고기, 새]

고양이 = [1, 0, 0, 0]
강아지 = [0, 1, 0, 0]
물고기 = [0, 0, 1, 0]
새     = [0, 0, 0, 1]
```

문제점:

- **차원의 저주**: 사전에 단어가 50,000개면 벡터 차원도 50,000이다. 대부분이 0인 극도로 비효율적인 표현이다
- **의미 관계 부재**: 고양이와 강아지는 의미적으로 가깝지만, one-hot 벡터 간의 거리는 모두 동일하다. 어떤 두 단어든 내적(dot product)이 항상 0이다
- **일반화 불가**: "고양이가 귀엽다"를 학습해도 "강아지가 귀엽다"로 일반화할 수 없다

### 2. Word Embedding이란?

각 단어를 **의미를 반영하는 저차원 실수 벡터**로 표현하는 방법이다.

```
고양이 = [0.21, -0.45, 0.83, ..., 0.12]  (예: 256차원)
강아지 = [0.19, -0.41, 0.79, ..., 0.15]  (고양이와 비슷!)
자동차 = [-0.62, 0.33, -0.11, ..., 0.87] (고양이와 다름)
```

좋은 임베딩의 특성:
- **비슷한 의미의 단어는 비슷한 벡터**를 가진다 (cosine similarity가 높다)
- 벡터 연산으로 의미 관계를 표현할 수 있다
- 유명한 예: `king - man + woman ≈ queen`

### 3. Word2Vec

단어 임베딩을 학습하는 대표적인 방법으로, 두 가지 변형이 있다.

**CBOW (Continuous Bag of Words)**
- 주변 단어들로 중심 단어를 예측한다
- "나는 [?]를 먹었다" -> [?] = "밥" 예측
- 주변 문맥이 주어졌을 때 가운데 단어를 맞추는 학습

**Skip-gram**
- 중심 단어로 주변 단어를 예측한다
- "밥" -> "나는", "를", "먹었다" 예측
- 하나의 단어가 주어졌을 때 어떤 단어가 주변에 올지 맞추는 학습

핵심 직관: **"비슷한 문맥에서 등장하는 단어는 비슷한 의미를 가진다"** (distributional hypothesis). "강아지가 뛰어다닌다"와 "고양이가 뛰어다닌다"에서 강아지와 고양이는 같은 문맥에 나타나므로 비슷한 벡터를 가지게 된다.

Word2Vec은 모델 학습과 별도로 대량의 텍스트에서 미리 학습(pretrained)되어 제공되는 경우가 많다.

### 4. nn.Embedding

PyTorch에서 임베딩을 구현하는 방법이다. 내부적으로는 **룩업 테이블(lookup table)**이다.

- 입력: 단어의 정수 인덱스 (예: "고양이" = 42)
- 출력: 해당 인덱스의 임베딩 벡터
- 파라미터: (사전 크기) x (임베딩 차원)의 행렬

작동 방식은 단순하다: 인덱스 42가 들어오면 행렬의 42번째 행을 그대로 반환한다. 이 행렬의 값은 학습을 통해 최적화된다.

One-hot 벡터와 선형 변환의 조합으로도 같은 결과를 얻을 수 있지만, nn.Embedding은 행렬 곱셈 없이 인덱스로 직접 접근하므로 훨씬 효율적이다.

### 5. Token Embeddings as LLM Input

현대 대형 언어 모델(LLM)에서 임베딩이 하는 역할을 이해하는 것이 중요하다.

**텍스트가 LLM에 입력되는 과정:**

1. **Tokenization**: 텍스트를 토큰(token) 단위로 분리한다. 현대 모델은 단어가 아닌 **서브워드(subword)** 단위를 사용한다. "unhappiness"는 "un" + "happiness" 또는 "un" + "happ" + "iness"로 분리될 수 있다
2. **인덱스 변환**: 각 토큰을 사전의 정수 인덱스로 변환한다
3. **Embedding lookup**: 각 인덱스를 nn.Embedding을 통해 벡터로 변환한다
4. **위치 정보 추가**: Positional encoding/embedding을 더해 순서 정보를 부여한다
5. **모델 입력**: 이 벡터 시퀀스가 Transformer 레이어에 입력된다

```
"빨간 컵을 집어"
    ↓ tokenization
["빨간", "컵", "을", "집", "어"]
    ↓ 인덱스 변환
[1042, 583, 12, 2947, 108]
    ↓ embedding lookup
[[0.21, ...], [0.45, ...], [0.11, ...], [0.83, ...], [0.37, ...]]
    ↓ + positional encoding
[[0.31, ...], [0.52, ...], [0.19, ...], [0.91, ...], [0.44, ...]]
    ↓
Transformer 레이어 입력
```

Word2Vec과 달리, LLM의 임베딩은 모델 전체와 함께 **end-to-end**로 학습된다. GPT 같은 모델의 임베딩 차원은 수천(예: 4096, 12288)에 달하며, 사전 크기도 수만~십만 이상이다.

---

## 연습 주제

1. One-hot encoding에서 두 단어 벡터의 내적이 항상 0인 이유를 설명하고, 이것이 왜 문제인지 서술하기
2. Word2Vec의 "king - man + woman = queen" 예시가 성립하는 이유를 distributional hypothesis 관점에서 설명하기
3. 서브워드 토크나이저가 단어 단위 토크나이저보다 나은 이유를 "처음 보는 단어(OOV, Out of Vocabulary)" 관점에서 정리하기
4. 임베딩 차원을 32로 설정하는 것과 4096으로 설정하는 것의 장단점을 비교하기
5. VLA에서 이미지가 입력될 때도 "임베딩"이 필요할지 생각해 보기 (힌트: vision encoder의 출력을 언어 모델 차원에 맞추는 과정)

---
