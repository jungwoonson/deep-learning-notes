# 인스트럭션 튜닝과 RLHF

## VLA와의 연결

VLA의 파인튜닝은 인스트럭션 튜닝의 로봇 버전이다. 이 평행 관계를 명확히 이해하면 VLA의 학습 방식이 자연스럽게 이해된다:

| 구성 요소 | 인스트럭션 튜닝 (LLM) | SFT on Robot Data (VLA) |
|----------|---------------------|------------------------|
| 입력 (Instruction) | 사용자 질문/명령 | 이미지 + 언어 명령 |
| 출력 (Response) | 텍스트 응답 | 로봇 행동 토큰 |
| 학습 데이터 | 사람이 작성한 질문-답변 쌍 | 사람이 수행한 로봇 데모 |
| 목표 | 유용한 응답 생성 | 올바른 행동 생성 |
| 학습 방식 | SFT (Supervised Fine-Tuning) | SFT (동일) |

OpenVLA가 로봇 데이터로 파인튜닝되는 과정은 본질적으로 "로봇 태스크 명령에 대해 올바른 행동으로 응답하는 인스트럭션 튜닝"이다.

---

## 핵심 개념

### 1. 사전학습된 LLM의 한계

GPT-3 같은 모델은 사전학습 후 놀라운 능력을 가지지만, **바로 쓸 수 있는 어시스턴트는 아니다.**

**문제점:**
- "대한민국의 수도는?"이라고 물으면 답을 하는 대신 "대한민국의 인구는? 대한민국의 면적은?" 같은 유사 질문을 이어서 생성할 수 있다
- 다음 토큰 예측으로 학습했기 때문에, "질문에 답한다"가 아니라 "인터넷에서 이어질 법한 텍스트를 생성한다"
- 유해한 내용, 편향된 정보, 거짓말을 자연스럽게 생성할 수 있다

**필요한 것:** 모델이 사용자의 "의도(intent)"를 이해하고, 그에 맞게 응답하도록 **정렬(alignment)**해야 한다.

### 2. 인스트럭션 튜닝 (Instruction Tuning) / SFT

#### 개념

사람이 만든 **(명령, 응답) 쌍** 데이터로 지도 학습을 수행한다. Supervised Fine-Tuning (SFT)라고도 한다.

#### 데이터 형태

```
[명령] 광합성 과정을 초등학생에게 설명해주세요.
[응답] 식물은 햇빛을 이용해서 물과 이산화탄소로 양분을 만들어요.
       이것을 광합성이라고 해요. 마치 식물이 요리를 하는 것과 같아요...
```

수천~수만 개의 이런 쌍을 모아 학습한다.

#### 대표적 인스트럭션 튜닝 모델

| 모델 | 기반 | 특징 |
|------|------|------|
| InstructGPT | GPT-3 | 최초의 체계적 인스트럭션 튜닝 |
| Alpaca | LLaMA 7B | GPT-3.5가 생성한 52K 인스트럭션으로 학습 |
| Vicuna | LLaMA 13B | ShareGPT 대화 데이터로 학습 |
| Llama 2-Chat | Llama 2 | Meta의 공식 인스트럭션 튜닝 버전 |

#### SFT의 학습 방식

- 기존 next token prediction과 동일한 손실 함수 (Cross-Entropy)
- 단, **명령 부분은 손실 계산에서 제외**하고, **응답 부분만** 학습
- 모델이 명령을 "읽고 이해"하되, 출력하는 법을 배우는 것은 응답 부분뿐

#### VLA에서의 SFT

OpenVLA의 SFT도 정확히 같은 구조이다:
- 입력: 이미지 토큰 + "pick up the red cup" (명령)
- 출력: 7개 행동 토큰
- 손실 함수: 이미지/명령 토큰은 제외, **행동 토큰에 대해서만** Cross-Entropy 계산

### 3. RLHF (Reinforcement Learning from Human Feedback)

SFT만으로는 부족한 경우가 있다. 두 응답이 모두 "정답"이지만 하나가 더 좋을 때, 이 **선호도(preference)**를 학습하는 방법이다.

#### RLHF 파이프라인 (3단계)

**Step 1: SFT (위에서 설명)**
- 인스트럭션 데이터로 기본적인 응답 능력 학습

**Step 2: Reward Model (보상 모델) 학습**
- 같은 명령에 대해 두 응답을 사람에게 보여주고 **어느 것이 더 좋은지** 선택하게 한다
- 이 선호도 데이터로 "응답의 품질을 점수화하는 모델"을 학습
- 입력: (명령, 응답) -> 출력: 스칼라 점수 (높을수록 좋음)

**Step 3: PPO (Proximal Policy Optimization)로 정책 최적화**
- SFT 모델을 "정책(policy)"으로 본다
- 모델이 응답을 생성하면, Reward Model이 점수를 매긴다
- 높은 점수를 받는 방향으로 정책을 업데이트
- **KL Divergence 제약**: SFT 모델에서 너무 벗어나지 않도록 제한 (reward hacking 방지)

#### 왜 RL이 필요한가?

- SFT는 "정답을 모방"하는 것. 하지만 좋은 응답의 기준은 다양하고 미묘함
- "이 응답이 좋다/나쁘다"를 직접 정의하기 어렵지만, "A가 B보다 낫다"는 비교는 쉬움
- Reward Model이 이 비교를 일반화하고, RL이 이를 최적화

### 4. DPO (Direct Preference Optimization)

RLHF의 PPO 단계를 **제거**하고, 선호도 데이터로 직접 모델을 학습하는 방법이다.

**RLHF의 문제점:**
- PPO는 구현이 복잡하다 (actor, critic, reward model, reference model -- 최대 4개 모델 동시 운용)
- 하이퍼파라미터에 민감하다
- 학습이 불안정하다

**DPO의 핵심 통찰:**
- Reward Model + PPO를 수학적으로 하나의 **분류 문제**로 환원할 수 있다
- 선호 응답(preferred)과 비선호 응답(dispreferred)을 직접 비교하여 학습
- "선호 응답의 확률은 높이고, 비선호 응답의 확률은 낮추되, 원래 모델에서 너무 벗어나지 않게"

**DPO의 장점:**
- Reward Model 학습 불필요
- RL 루프 불필요
- 일반적인 SFT와 비슷한 학습 파이프라인
- 안정적이고 구현이 간단

**Llama 2-Chat은 RLHF(PPO)를 사용**, 최근 모델들은 DPO를 더 많이 사용하는 추세.

### 5. Alignment의 세 가지 축

InstructGPT 논문이 제시한 정렬의 기준:

| 축 | 의미 | 예시 |
|----|------|------|
| **Helpful** (유용성) | 사용자의 질문에 도움이 되는가 | 정확하고 상세한 답변 |
| **Honest** (진실성) | 사실에 기반하고, 불확실성을 인정하는가 | "모르겠습니다"라고 말할 수 있는 능력 |
| **Harmless** (무해성) | 유해한 내용을 생성하지 않는가 | 위험한 지시를 거부 |

### 6. VLA에서의 Alignment 평행 관계

VLA에도 정렬(alignment)이 필요하다:

| LLM Alignment | VLA Alignment |
|--------------|---------------|
| Helpful: 유용한 답변 | **Effective**: 태스크를 성공적으로 수행 |
| Honest: 사실에 기반 | **Calibrated**: 불확실할 때 조심스럽게 행동 |
| Harmless: 유해하지 않음 | **Safe**: 물건을 부수거나 사람을 다치게 하지 않음 |

**현재 VLA의 정렬 방식:**
- 주로 SFT만 사용 (사람의 데모 데이터로 학습)
- RLHF/DPO에 해당하는 "로봇 행동 선호도 학습"은 아직 초기 연구 단계
- 향후: 사람이 두 로봇 행동 중 더 나은 것을 선택 -> Reward Model -> RL로 행동 정책 개선

### 7. 학습 단계 요약: Base Model -> Chat Model

```
[사전학습된 Base Model]
  "대한민국의 수도는?" -> "대한민국의 인구는? 대한민국의..."
          |
          v  SFT (인스트럭션 튜닝)
[SFT Model]
  "대한민국의 수도는?" -> "대한민국의 수도는 서울입니다."
          |
          v  RLHF 또는 DPO
[Chat Model]
  "대한민국의 수도는?" -> "대한민국의 수도는 서울입니다.
                          서울은 한반도 중앙부에 위치하며..."
                          (더 자세하고, 안전하며, 사용자 의도에 맞는 응답)
```

---

## 연습 주제 (코드 없이 생각해보기)

1. **SFT 데이터 설계**: VLA를 위한 SFT 데이터를 설계한다면, 하나의 데이터 포인트는 어떤 구성을 가져야 하는가? (입력, 출력, 손실 계산 범위를 명확히 하라)

2. **Reward Model for Robots**: 로봇 행동의 Reward Model을 학습한다고 가정하라. "이 행동이 좋은가?"를 판단하는 보상 함수는 어떻게 설계하겠는가? 사람의 피드백을 어떻게 수집할 수 있을까?

3. **RLHF vs DPO**: RLHF(PPO 기반)와 DPO의 장단점을 비교하라. 로봇 행동 학습에는 어느 것이 더 적합하다고 생각하는가? 그 이유는?

4. **Alignment 실패 사례**: VLA에서 alignment가 실패하면 어떤 결과가 나타날 수 있는지 구체적 시나리오를 3가지 제시하라 (helpful, honest, harmless 축 각각에서).

5. **SFT의 한계**: 인스트럭션 튜닝(SFT)만으로는 왜 충분하지 않은가? "평균적인 인간 데모를 모방"하는 것의 한계를 로봇 맥락에서 설명하라.

---
