# 토크나이징과 BPE

## VLA와의 연결

VLA에서 토크나이저는 단순히 텍스트를 나누는 도구가 아니다. **OpenVLA는 기존 언어 토크나이저의 어휘(vocabulary)를 확장하여 로봇 행동(action)을 표현하는 새로운 토큰을 추가한다.** 구체적으로:

- 기존 Llama 2 토크나이저: 텍스트를 토큰으로 변환
- OpenVLA 확장: 연속적인 로봇 행동값(예: 관절 각도)을 **256개 구간으로 양자화(discretize)**하여 각 구간을 하나의 토큰으로 표현
- 7-DoF 로봇 팔의 행동 = 7개의 행동 토큰 시퀀스

토크나이저를 이해해야 이 "행동의 토큰화"가 왜 가능하고, 어떤 트레이드오프가 있는지 파악할 수 있다.

---

## 핵심 개념

### 1. 왜 토크나이징이 필요한가

신경망은 숫자만 처리할 수 있다. 텍스트를 숫자로 변환하는 과정이 토크나이징이다.

**텍스트 -> 토큰 시퀀스 -> 정수 ID 시퀀스 -> 임베딩 벡터**

토크나이징 방식에 따라 모델의 성능, 어휘 크기, 처리 효율이 크게 달라진다.

### 2. 세 가지 토크나이징 수준

#### Character-level (문자 단위)

- `"hello"` -> `["h", "e", "l", "l", "o"]`
- **장점**: 어휘 크기가 매우 작다 (영어는 약 256개면 충분)
- **단점**: 시퀀스가 길어진다. "transformer"는 11개 토큰. 긴 시퀀스는 attention 계산 비용이 급격히 증가 (O(n^2))

#### Word-level (단어 단위)

- `"I love cats"` -> `["I", "love", "cats"]`
- **장점**: 직관적, 시퀀스가 짧다
- **단점**: 어휘 크기가 폭발한다. 영어만 수십만 단어. "unhappiness" 같은 파생어도 별개 토큰. 학습 데이터에 없던 단어는 `<UNK>` (unknown) 처리 -- OOV(Out-of-Vocabulary) 문제

#### Subword-level (서브워드 단위) -- 현대 LLM의 표준

- `"unhappiness"` -> `["un", "happiness"]` 또는 `["un", "happi", "ness"]`
- **장점**: 어휘 크기를 제어할 수 있다. 처음 보는 단어도 서브워드 조합으로 표현 가능. OOV 문제 해결
- **핵심 아이디어**: 자주 나오는 문자열은 하나의 토큰으로, 드문 문자열은 여러 토큰으로 분해

### 3. BPE (Byte Pair Encoding)

현재 가장 널리 쓰이는 서브워드 토크나이징 알고리즘. GPT 시리즈가 사용한다.

**BPE 어휘 구축 과정:**

1. **초기화**: 모든 개별 문자를 기본 어휘로 설정
2. **빈도 계산**: 학습 데이터에서 가장 자주 붙어 나오는 문자 쌍(pair)을 찾는다
3. **병합(merge)**: 그 쌍을 하나의 새로운 토큰으로 병합하고 어휘에 추가
4. **반복**: 원하는 어휘 크기에 도달할 때까지 2-3을 반복

**예시 (간략화):**

학습 데이터에 `"low"`, `"lower"`, `"newest"`, `"widest"`가 있다고 하자.

- 초기 어휘: `{l, o, w, e, r, n, s, t, i, d}`
- 가장 빈번한 쌍: `(e, s)` -> 병합 -> `es` 추가
- 다음 빈번한 쌍: `(es, t)` -> 병합 -> `est` 추가
- 다음: `(l, o)` -> 병합 -> `lo` 추가
- 계속 반복...

**결과:** 자주 나오는 패턴(`est`, `lo`, `er` 등)이 하나의 토큰이 되고, 드문 조합은 더 작은 단위로 분해됨.

### 4. Byte-level BPE

GPT-2부터 사용된 변형. 기본 단위를 문자가 아닌 **바이트(byte)**로 한다.

- 모든 텍스트는 UTF-8 바이트 시퀀스로 표현 가능
- 기본 어휘가 256개(0~255)로 고정
- **어떤 언어든, 어떤 특수 문자든** 표현 가능 -- `<UNK>` 토큰이 원천적으로 불필요
- Llama 2도 Byte-level BPE를 사용

### 5. SentencePiece

Google이 개발한 토크나이저 라이브러리. BPE 또는 Unigram 알고리즘을 구현한다.

**핵심 특징:**
- **언어 독립적**: 사전 토크나이징(공백 분리 등) 없이 원본 텍스트에서 바로 학습
- 한국어, 일본어, 중국어 등 공백이 단어 경계가 아닌 언어에 특히 유용
- Llama 시리즈가 SentencePiece를 사용

**Unigram 모델 (SentencePiece의 또 다른 옵션):**
- BPE와 반대 방향: 큰 어휘에서 시작하여 불필요한 토큰을 제거해 나감
- 각 토크나이징 결과에 확률을 부여하여 최적의 분할을 선택
- T5 등이 사용

### 6. Vocabulary Size (어휘 크기)의 영향

| 어휘 크기 | 장점 | 단점 |
|----------|------|------|
| 작음 (예: 8K) | 임베딩 파라미터 적음, 메모리 절약 | 시퀀스가 길어짐, 의미 단위가 쪼개짐 |
| 큼 (예: 128K) | 시퀀스가 짧아짐, 의미 단위 보존 | 임베딩 테이블이 커짐, 드문 토큰 학습 부족 |

**대표 모델들의 어휘 크기:**
- GPT-2: 50,257
- Llama 2: 32,000
- GPT-4: 약 100,000

### 7. VLA에서의 토크나이저 확장

OpenVLA는 Llama 2의 32,000개 어휘에 **256개 행동 토큰**을 추가한다.

**행동 토큰화 과정:**
1. 연속적인 행동 값(예: 관절 각도 -1.0 ~ +1.0)을 256개 구간(bin)으로 양자화
2. 각 구간에 고유한 토큰 ID를 부여 (32,000 ~ 32,255)
3. 7-DoF 행동 = 7개의 행동 토큰 시퀀스

**트레이드오프:**
- 256개 bin: 각 축의 해상도는 약 0.8% -> 대부분의 로봇 작업에 충분
- bin 수를 늘리면 해상도는 높아지지만 어휘가 커지고 학습이 어려워짐
- bin 수를 줄이면 정밀한 동작이 불가능

---

## 연습 주제 (코드 없이 생각해보기)

1. **BPE 수동 실행**: `["ab ab cb", "ab cb ab"]` 데이터로 BPE 병합을 3단계까지 손으로 수행하라. 각 단계에서 어떤 쌍이 선택되고 어휘가 어떻게 변하는지 기록하라.

2. **토크나이징 수준 비교**: `"understanding"`이라는 단어를 character, word, subword 수준으로 각각 토크나이징한 결과를 적고, 각 방식의 시퀀스 길이와 어휘 요구량을 비교하라.

3. **어휘 크기 트레이드오프**: 한국어 텍스트를 처리할 때 어휘 크기 8K vs 64K의 차이를 생각해 보라. 한국어는 조사/어미 활용이 풍부한데, 이것이 어떤 영향을 미칠까?

4. **행동 양자화**: 로봇 팔의 한 관절이 -180도에서 +180도 범위를 가진다. 256개 bin으로 양자화하면 한 bin의 해상도는 몇 도인가? 이 해상도로 정밀 조립 작업이 가능하겠는가?

5. **토크나이저 확장 설계**: VLA 토크나이저에 행동 토큰 외에 "센서 값 토큰"도 추가하고 싶다면 어떻게 설계하겠는가? 어휘 확장의 한계는 무엇인가?

---

## 다음 노트

[사전학습과 파인튜닝](./04-pretraining-finetuning.md) -- LLM이 어떻게 범용 지식을 먼저 학습하고, 특정 태스크에 적응하는지. VLA에서 이 패러다임이 어떻게 적용되는지.
