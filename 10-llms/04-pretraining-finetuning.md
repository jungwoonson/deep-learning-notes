# 사전학습과 파인튜닝

## VLA와의 연결

OpenVLA는 **pre-train then fine-tune 패러다임의 정수**이다:

1. **사전학습 (Pre-training)**: Prismatic VLM을 대규모 이미지-텍스트 데이터로 학습 -> 시각과 언어에 대한 범용 이해력 확보
2. **파인튜닝 (Fine-tuning)**: Open X-Embodiment 로봇 데이터로 학습 -> 시각+언어 이해력을 로봇 행동 생성으로 전이

이 패러다임의 핵심 질문 -- "얼마나 큰 모델을, 얼마나 많은 데이터로, 얼마나 오래 사전학습해야 하는가?" -- 은 Scaling Laws로 답할 수 있으며, VLA의 설계 결정에도 직접 영향을 미친다.

---

## 핵심 개념

### 1. Pre-train Then Fine-tune 패러다임

현대 AI의 지배적 학습 방식이다.

**전통적 방식 (태스크별 학습):**
- 각 태스크마다 처음부터 모델을 학습
- 감성 분석 모델, 번역 모델, QA 모델 ... 모두 별도
- 비효율적: 매번 언어 자체를 다시 배워야 함

**Pre-train + Fine-tune:**
- **Phase 1 - Pre-training**: 대규모 비지도 데이터로 "언어란 무엇인가"를 학습
- **Phase 2 - Fine-tuning**: 소규모 태스크별 데이터로 특정 능력을 학습
- 효율적: 언어 이해는 한 번만, 태스크 적응은 빠르게

**비유:** 사전학습 = 대학교 일반 교육, 파인튜닝 = 직무 교육

### 2. 사전학습의 두 가지 방식

#### 자기지도 학습 (Self-Supervised Learning)

레이블 없이 데이터 자체에서 학습 신호를 만든다:

- **MLM (BERT 방식)**: 마스킹된 토큰 복원
- **Next Token Prediction (GPT 방식)**: 다음 토큰 예측
- **Contrastive Learning (CLIP 방식)**: 이미지-텍스트 쌍의 매칭/불일치 판별

레이블이 필요 없으므로 인터넷의 모든 텍스트를 학습 데이터로 사용 가능. 이것이 스케일의 핵심.

#### 사전학습 데이터 규모

| 모델 | 학습 데이터 |
|------|-----------|
| BERT | 약 3.3B 단어 (BookCorpus + Wikipedia) |
| GPT-3 | 약 300B 토큰 (CommonCrawl, WebText 등) |
| Llama 2 | 약 2T 토큰 |
| Llama 3 | 약 15T 토큰 |

### 3. 파인튜닝의 방식들

#### Full Fine-tuning (전체 파인튜닝)

- 사전학습된 모델의 **모든 파라미터**를 태스크 데이터로 업데이트
- 가장 강력하지만 비용이 크다
- 모든 태스크마다 전체 모델의 복사본이 필요

#### Feature Extraction (특성 추출)

- 사전학습된 모델은 **고정(freeze)**하고, 마지막에 새 레이어만 학습
- 가장 가볍지만 성능이 제한적

#### Parameter-Efficient Fine-tuning (PEFT)

- 모델 대부분은 고정하고, **소수의 파라미터만** 학습
- LoRA, Adapter 등 (별도 노트에서 상세히 다룸)
- OpenVLA가 이 방식을 사용

### 4. Scaling Laws (스케일링 법칙)

2020년 OpenAI의 Kaplan et al. 논문이 발견한 경험적 법칙이다. 모델 성능(loss)은 세 가지 요소의 **거듭제곱 법칙(power law)**을 따른다:

**세 가지 축:**
- **N**: 모델 파라미터 수
- **D**: 학습 데이터 토큰 수
- **C**: 총 연산량 (compute, FLOPs)

**핵심 발견:**
- Loss는 N, D, C 각각에 대해 **매끄러운 거듭제곱 곡선**을 따라 감소
- 이 세 가지 중 하나라도 부족하면 병목이 된다
- 세 가지를 균형 있게 키우는 것이 최적

**Chinchilla Scaling (2022, DeepMind):**
- Kaplan의 법칙을 수정: 데이터가 더 중요하다
- **최적 비율**: 모델 파라미터 수 대비 약 20배의 학습 토큰이 필요
- 예: 70B 모델은 약 1.4T 토큰으로 학습해야 최적
- 이전에는 모델만 키우고 데이터는 상대적으로 적었음 -> 비효율

### 5. Scaling Laws의 실제 영향

**Compute-Optimal Training (연산 최적 학습):**

고정된 연산 예산이 있을 때:
- **과거**: 큰 모델을 적은 데이터로 학습 (GPT-3 방식)
- **현재**: 적절한 크기의 모델을 충분한 데이터로 학습 (Chinchilla 방식)

| 모델 | 파라미터 | 학습 토큰 | 접근법 |
|------|---------|----------|--------|
| GPT-3 | 175B | 300B | Under-trained |
| Chinchilla | 70B | 1.4T | Compute-optimal |
| Llama 2 | 70B | 2T | Over-trained (추론 효율 위해) |

**Over-training**: Llama 2는 의도적으로 최적보다 더 많은 데이터로 학습. 사전학습 비용은 증가하지만, 배포 후 추론 비용이 더 중요하므로 더 작은 모델을 더 잘 학습시키는 전략.

### 6. VLA에서의 사전학습/파인튜닝

**OpenVLA의 학습 파이프라인:**

```
[Phase 0] 개별 모델 사전학습
  - SigLIP: 이미지-텍스트 쌍으로 vision encoder 학습
  - Llama 2 7B: 대규모 텍스트로 언어 모델 학습

[Phase 1] VLM 사전학습 (Prismatic)
  - SigLIP + Llama 2를 결합하여 이미지-텍스트 태스크 학습
  - 시각과 언어를 연결하는 projector 학습

[Phase 2] VLA 파인튜닝
  - Prismatic VLM에 행동 토큰 추가
  - Open X-Embodiment 로봇 데이터로 파인튜닝
  - "이미지 + 명령 -> 행동 토큰" 생성 학습
```

**핵심 관찰:**
- 각 단계가 이전 단계의 지식을 계승
- 로봇 데이터만으로는 시각이나 언어를 배울 수 없다 (데이터가 너무 적으므로)
- 사전학습이 없다면 수조 개의 로봇 데이터가 필요할 것

### 7. 전이 학습 (Transfer Learning)의 핵심 가정

Pre-train + Fine-tune가 작동하는 근본적 이유:

- **저수준 특성의 보편성**: 언어의 문법, 의미론, 논리 구조는 태스크에 관계없이 유사
- **계층적 표현**: 초기 레이어는 범용적, 후기 레이어는 태스크 특화
- **데이터 효율성**: 범용 지식이 있으면 적은 태스크 데이터로도 빠르게 적응

**VLA에서의 전이:**
- 사전학습된 vision encoder는 이미 물체를 인식할 수 있다
- 사전학습된 언어 모델은 이미 명령을 이해할 수 있다
- 파인튜닝은 이 이해력을 "어떻게 행동할까"로 연결하는 것만 학습

---

## 연습 주제 (코드 없이 생각해보기)

1. **패러다임 비교**: "처음부터 학습" vs "pre-train + fine-tune"를 사람의 교육에 비유하라. 각각 어떤 상황에 해당하는가?

2. **Scaling Laws 계산**: 연산 예산이 10배 늘어났다고 하자. Chinchilla 법칙에 따르면 모델 크기와 데이터 크기를 각각 어떻게 키워야 하는가? 단순히 모델만 10배 키우면 어떤 문제가 생기는가?

3. **데이터 병목**: VLA에서 가장 부족한 자원은 무엇인가? 로봇 데이터의 Scaling Law는 텍스트 데이터의 Scaling Law와 어떻게 다를 수 있을까?

4. **파인튜닝 전략 선택**: 다음 시나리오 각각에서 full fine-tuning, feature extraction, PEFT 중 어느 것이 적합한지 판단하라:
   - 7B 모델을 1,000개 로봇 데모로 파인튜닝
   - 7B 모델을 1,000,000개 로봇 데모로 파인튜닝
   - 새로운 로봇 플랫폼에 빠르게 적응

5. **Catastrophic Forgetting**: 파인튜닝 과정에서 사전학습된 지식이 손실될 수 있다 (Catastrophic Forgetting). VLA에서 이것이 문제가 된다면 어떤 증상으로 나타날까? 이미지를 인식하지 못하게 된다면?

---

## 다음 노트

[Llama 아키텍처](./05-llama-architecture.md) -- OpenVLA의 실제 백본인 Llama 2의 구체적 아키텍처 구성 요소. RMSNorm, SwiGLU, RoPE, GQA를 깊이 있게 다룬다.
