# 대규모 언어 모델

Transformer 기반의 언어 모델을 다룬다. BERT, GPT 계열의 구조 차이부터 토크나이저, 사전학습/파인튜닝, RLHF, LoRA까지 LLM의 전체 파이프라인을 정리한다.

| # | 주제 | 상태 |
|---|------|------|
| 1 | [BERT](01-bert-encoder-models.md) | |
| 2 | [GPT (자기회귀)](02-gpt-autoregressive.md) | |
| 3 | [토크나이징 (BPE)](03-tokenization-bpe.md) | |
| 4 | [사전학습/파인튜닝](04-pretraining-finetuning.md) | |
| 5 | [Llama 아키텍처](05-llama-architecture.md) | |
| 6 | [인스트럭션 튜닝/RLHF](06-instruction-tuning-rlhf.md) | |
| 7 | [LoRA/PEFT](07-lora-peft.md) | |
