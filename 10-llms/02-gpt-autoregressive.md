# GPT와 Autoregressive 모델

## VLA와의 연결

**OpenVLA의 핵심 백본은 Llama 2이며, Llama 2는 GPT 계열의 decoder-only autoregressive 모델이다.** VLA가 로봇 행동을 생성하는 방식은 GPT가 텍스트를 생성하는 방식과 본질적으로 동일하다:

- GPT: 이전 토큰들 -> 다음 텍스트 토큰 예측
- VLA: 이미지 + 언어 명령 + 이전 행동 토큰 -> 다음 행동 토큰 예측

이 노트에서 다루는 autoregressive 생성과 디코딩 전략은 VLA의 행동 생성 메커니즘을 이해하는 데 직접적으로 적용된다.

---

## 핵심 개념

### 1. Decoder-Only 아키텍처

GPT는 Transformer의 **decoder 부분만** 사용한다. BERT의 encoder와 결정적으로 다른 점은 **Causal Masking**이다.

**Causal Masking (인과적 마스킹):**
- 각 토큰은 자신과 자신 **이전**의 토큰만 참조할 수 있다
- 미래 토큰에 대한 attention weight를 `-infinity`로 설정하여 softmax 후 0이 되게 함
- 이것이 "왼쪽에서 오른쪽으로" 순차적 생성을 가능하게 한다

왜 이렇게 하는가? 생성 시 미래 토큰은 아직 존재하지 않기 때문이다. 학습 때부터 미래를 못 보게 해야 생성 시와 조건이 일치한다.

### 2. Autoregressive 생성

"Autoregressive"는 **자기 자신의 이전 출력을 다음 입력으로 사용**한다는 뜻이다.

**생성 과정 (한 단계씩):**

1. 입력: `"오늘 날씨가"`
2. 모델 출력: 다음 토큰의 확률 분포 -> `"좋다"` 선택
3. 입력 갱신: `"오늘 날씨가 좋다"`
4. 모델 출력: 다음 토큰의 확률 분포 -> `"."` 선택
5. 입력 갱신: `"오늘 날씨가 좋다."`
6. `<EOS>` (End of Sequence) 토큰이 나올 때까지 반복

**핵심:** 모델은 한 번에 하나의 토큰만 생성한다. 전체 문장은 이 과정의 반복으로 만들어진다.

**학습 시의 효율적 트릭 - Teacher Forcing:**
- 생성 시에는 한 토큰씩 순차 생성하지만, 학습 시에는 정답 시퀀스를 한번에 넣고 모든 위치에서 동시에 다음 토큰을 예측
- Causal mask 덕분에 각 위치는 미래를 볼 수 없어, 한번의 forward pass로 모든 위치를 학습 가능

### 3. GPT의 진화

| 모델 | 파라미터 수 | 핵심 변화 |
|------|-----------|----------|
| GPT-1 | 117M | Transformer decoder + 비지도 사전학습 |
| GPT-2 | 1.5B | 스케일 업, zero-shot 능력 발견 |
| GPT-3 | 175B | few-shot learning, in-context learning |
| GPT-4 | 비공개 | 멀티모달 (이미지 입력 가능) |

**핵심 발견:** 모델을 크게 만들면 명시적 fine-tuning 없이도 프롬프트만으로 다양한 태스크를 수행할 수 있다 (in-context learning). 하지만 VLA에서는 여전히 fine-tuning이 필요하다 -- 로봇 행동은 텍스트와 본질적으로 다른 도메인이기 때문.

### 4. 텍스트 생성 전략 (Decoding Strategies)

모델은 매 스텝 다음 토큰에 대한 **확률 분포**를 출력한다. 이 분포에서 실제 토큰을 어떻게 선택하느냐가 생성 전략이다.

#### Greedy Decoding

- 항상 **가장 높은 확률의 토큰**을 선택
- 단순하고 결정적(deterministic)
- 문제: 반복적이고 지루한 텍스트를 생성하는 경향

#### Sampling

- 확률 분포에 따라 **무작위 샘플링**
- 다양성은 높지만, 낮은 확률의 이상한 토큰이 선택될 위험

#### Temperature

- 확률 분포의 **날카로움/부드러움**을 조절하는 하이퍼파라미터
- softmax 전 logit을 temperature T로 나눈다: `logit / T`
- **T < 1**: 분포가 날카로워짐 -> 높은 확률 토큰에 집중 (더 결정적)
- **T = 1**: 원래 분포 그대로
- **T > 1**: 분포가 평탄해짐 -> 다양성 증가 (더 무작위)
- **T -> 0**: greedy decoding과 동일

#### Top-k Sampling

- 확률 상위 **k개** 토큰만 후보로 두고, 이 중에서 샘플링
- k = 1이면 greedy, k가 크면 더 다양
- 문제: 적절한 k 값이 상황마다 다름. 어떤 위치에서는 2~3개가 합리적이고, 다른 위치에서는 수십 개가 합리적

#### Top-p (Nucleus) Sampling

- 누적 확률이 **p**를 넘는 최소한의 토큰 집합에서 샘플링
- 예: p = 0.9이면 확률 합이 90%가 되는 최소 토큰들만 사용
- Top-k보다 적응적: 확률 분포가 집중되면 적은 후보, 분산되면 많은 후보
- 실무에서 가장 널리 사용

#### VLA에서의 디코딩

OpenVLA는 행동 토큰을 생성할 때 **greedy decoding**을 사용한다. 로봇 행동은 "창의적 다양성"보다 **정확성과 일관성**이 중요하기 때문이다. 같은 상황에서 매번 다른 행동을 하면 안 된다.

### 5. GPT의 학습 목표: Next Token Prediction

GPT의 학습은 놀랍도록 단순하다:

- **목표**: 주어진 이전 토큰들로 다음 토큰을 예측
- **손실 함수**: Cross-Entropy Loss (정답 토큰의 확률을 최대화)
- 이것이 전부이다. 이 단순한 목표로 문법, 사실, 추론 능력이 모두 학습된다

VLA도 동일하다: 이미지 토큰 + 언어 토큰이 주어지면 다음 행동 토큰을 예측. 손실 함수도 cross-entropy.

---

## 연습 주제 (코드 없이 생각해보기)

1. **Causal Mask 그리기**: 4개 토큰 `[A, B, C, D]`에 대해 4x4 attention mask를 직접 그려보라. 각 칸이 "참조 가능(1)" 또는 "참조 불가(0)"인지 표시하라. BERT는 같은 4개 토큰에 대해 어떤 mask를 사용하겠는가?

2. **디코딩 비교**: 다음 토큰 확률 분포가 `{고양이: 0.4, 강아지: 0.3, 새: 0.15, 물고기: 0.1, 자동차: 0.05}`일 때, 다음 각 전략의 결과를 예측하라:
   - Greedy
   - Top-k (k=3)
   - Top-p (p=0.7)
   - Top-p (p=0.9)

3. **Temperature 효과**: 위 분포에 temperature 0.5와 2.0을 적용하면 각각 어떻게 변할지 정성적으로 설명하라.

4. **VLA 연결**: OpenVLA가 행동 생성에 greedy decoding을 사용하는 이유를 설명하라. 만약 top-p sampling을 사용하면 로봇에게 어떤 문제가 생길 수 있겠는가?

5. **Autoregressive의 한계**: autoregressive 생성은 왜 병렬화가 어려운가? 이것이 실시간 로봇 제어에 어떤 영향을 미칠 수 있겠는가?

---
