# BERT와 Encoder-Only 모델

## VLA와의 연결

VLA(Vision-Language-Action) 파이프라인에서 주로 사용되는 것은 **decoder-only** 모델(GPT 계열)이다. 그렇다면 BERT는 왜 배우는가? BERT가 도입한 **pre-training + fine-tuning 패러다임**이 LLM 시대 전체의 기반이 되었기 때문이다. VLA도 결국 "사전학습된 모델을 로봇 데이터로 파인튜닝"하는 같은 패턴을 따른다. BERT를 이해하면 이 패러다임의 출발점을 이해하게 된다.

---

## 핵심 개념

### 1. Encoder-Only 아키텍처

Transformer는 원래 encoder + decoder 구조로 설계되었다. BERT는 이 중 **encoder 부분만** 사용한다.

- **Encoder**: 입력 시퀀스 전체를 한꺼번에 읽고, 각 토큰에 대한 **문맥화된 표현(contextualized representation)**을 출력
- 모든 토큰이 다른 모든 토큰을 참조할 수 있다 (양방향)
- 출력은 입력과 같은 길이의 벡터 시퀀스

핵심 차이를 정리하면:

| 구분 | Encoder-Only (BERT) | Decoder-Only (GPT) |
|------|-------------------|-------------------|
| Attention 방향 | 양방향 (Bidirectional) | 단방향 (Causal, 왼쪽만) |
| 주요 용도 | 이해 (분류, NER 등) | 생성 (텍스트 생성) |
| 입출력 | 입력 전체 -> 표현 벡터 | 이전 토큰 -> 다음 토큰 |

### 2. Bidirectional Context (양방향 문맥)

"나는 은행에 갔다"에서 "은행"이 금융기관인지 강둑인지 파악하려면 앞뒤 문맥이 모두 필요하다.

- GPT 방식 (단방향): "나는" -> "은행" 예측. "갔다"를 못 본다
- BERT 방식 (양방향): "나는 ___ 에 갔다" 전체를 보고 빈칸을 예측

BERT의 Self-Attention은 **모든 위치 쌍** 사이에 attention을 계산한다. 각 토큰은 시퀀스 내 모든 다른 토큰의 정보를 받을 수 있다.

### 3. MLM (Masked Language Modeling) Pre-training

BERT의 사전학습 방식이다. 레이블이 없는 대규모 텍스트만으로 학습한다.

**방법:**
- 입력 토큰의 약 15%를 무작위로 선택
- 선택된 토큰 중 80%는 `[MASK]`로 교체, 10%는 랜덤 토큰으로 교체, 10%는 유지
- 모델은 원래 토큰이 무엇이었는지 맞춰야 함

**왜 이렇게 하는가?**
- 단순히 100% 마스킹하면 fine-tuning 시 `[MASK]` 토큰이 없어 불일치(mismatch)가 생김
- 80/10/10 비율로 이 문제를 완화

**추가 목표 - NSP (Next Sentence Prediction):**
- 두 문장이 실제로 연속되는지 예측
- 후속 연구(RoBERTa)에서 NSP가 크게 도움이 안 된다는 것이 밝혀짐

### 4. Fine-tuning (파인튜닝)

사전학습된 BERT 위에 **작은 출력 레이어**를 추가하고, 특정 태스크의 레이블 데이터로 전체 모델을 미세 조정한다.

**대표적 Fine-tuning 태스크:**
- **문장 분류 (Sentence Classification)**: `[CLS]` 토큰의 출력 벡터 -> 분류기
- **토큰 분류 (Token Classification)**: 각 토큰 출력 -> 개체명 인식(NER) 등
- **질의응답 (Question Answering)**: 답이 되는 구간의 시작/끝 위치 예측

**핵심 인사이트**: 사전학습으로 언어 자체를 이해하고, 파인튜닝으로 특정 작업에 적응. 이 패턴이 VLA에서도 반복된다 -- VLM으로 시각+언어를 이해한 뒤, 로봇 데이터로 파인튜닝하여 행동을 출력.

### 5. BERT 이후의 발전

| 모델 | 핵심 변경점 |
|------|-----------|
| RoBERTa | NSP 제거, 더 많은 데이터, 더 오래 학습 |
| ALBERT | 파라미터 공유로 모델 경량화 |
| DeBERTa | Disentangled Attention (내용과 위치를 분리) |
| ELECTRA | MLM 대신 Replaced Token Detection |

---

## 연습 주제 (코드 없이 생각해보기)

1. **MLM 시뮬레이션**: 한국어 문장 하나를 골라 15% 토큰을 마스킹하고, 양방향 문맥만으로 원래 토큰을 추론해 보라. 단방향(왼쪽만)으로 시도하면 어떤 차이가 있는가?

2. **아키텍처 선택**: 다음 태스크 각각에 대해 encoder-only와 decoder-only 중 어느 것이 더 적합한지, 그리고 그 이유를 설명하라.
   - 스팸 이메일 분류
   - 챗봇 응답 생성
   - 감성 분석
   - 번역

3. **Fine-tuning 설계**: BERT로 "로봇 명령어가 유효한지 판별하는 분류기"를 만든다고 가정하라. 입력은 무엇이고, 출력 레이어는 어떻게 설계하겠는가? `[CLS]` 토큰은 어떤 역할을 하는가?

4. **Pre-training 데이터**: MLM에서 마스킹 비율을 50%로 올리면 어떤 문제가 생길지, 반대로 5%로 낮추면 어떤 문제가 생길지 생각해 보라.

---
