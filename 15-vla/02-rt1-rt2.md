# RT-1 / RT-2: VLA의 기원

## VLA와의 연결

**RT-1과 RT-2는 VLA 패러다임의 탄생을 이해하는 출발점이다.** RT-1은 "대규모 로봇 데이터 + Transformer"로 범용 로봇 정책의 가능성을 보여줬지만, 학습 데이터에 없는 물체에 대한 일반화가 부족했다. RT-2는 이 한계를 **사전학습된 VLM(Vision-Language Model)의 지식을 로봇에 전이**하는 방법으로 돌파했다. 이 "VLM 파인튜닝" 아이디어가 이후 OpenVLA, pi-0, Helix 등 모든 VLA의 공통 원리가 되었다.

---

## 핵심 개념

### 1. RT-1 (Robotics Transformer 1, 2022.12)

#### 배경과 동기

2022년 이전의 로봇 학습:
- 각 task마다 별도의 모델을 학습
- 수백~수천 개의 데모 데이터로 하나의 task만 수행
- "컵 집기" 모델과 "서랍 열기" 모델이 별개

Google DeepMind의 질문: **하나의 모델이 수백 가지 task를 수행할 수 있을까?**

#### RT-1의 아키텍처

```
RT-1 아키텍처:

카메라 이미지 (300x300)
      ↓
EfficientNet-B3 (이미지 인코더)
      ↓
이미지 토큰 (8개 토큰)
      +
자연어 명령 토큰 (Universal Sentence Encoder)
      ↓
Token Learner (토큰 수 압축)
      ↓
Transformer Decoder (8개 자기회귀 레이어)
      ↓
이산화된 행동 (discretized actions)
  - 7-DoF 팔: x, y, z, rx, ry, rz, gripper
  - 3-DoF 이동: x, y, yaw
  - 각 차원 256 bins으로 이산화
```

#### RT-1의 데이터와 성과

```
학습 데이터:
  - 13만 에피소드 (130K episodes)
  - 17개월 간 13대의 로봇으로 수집
  - 실제 주방 환경에서 텔레오퍼레이션
  - 700+ 종류의 task

성과:
  - 학습한 task에 대해 97% 성공률
  - 단일 모델로 700+ task 수행
  - 이전 방법 대비 25% 성능 향상

한계 (핵심!):
  - 학습에 없는 물체에 대한 일반화 부족
    예: "사과를 집어라" 학습 → "레몬을 집어라" 실패
  - 배경이 바뀌면 성능 하락
  - 로봇 전용 데이터만 사용 → 세상 지식 부족
  - 의미적 추론 불가
    예: "탄산음료를 집어라" → 콜라가 탄산음료인지 모름
```

### 2. RT-2 (Robotics Transformer 2, 2023.07)

#### 핵심 통찰: VLM을 로봇에 활용

RT-2의 돌파구는 단순하지만 강력하다:

```
RT-1의 접근:
  로봇 데이터만으로 처음부터 학습 → 세상 지식 부족

RT-2의 접근:
  인터넷 데이터로 사전학습된 VLM을 로봇 데이터로 파인튜닝
  → VLM이 이미 "레몬은 노란 과일이다"를 알고 있음
  → "노란 과일을 집어라" 명령 시 레몬을 인식할 수 있음
```

이것이 혁명적인 이유:
- VLM은 인터넷의 수십억 장 이미지와 텍스트로 학습됨
- "세상에 대한 이해" (semantic knowledge)가 이미 내장됨
- 로봇 데이터로 파인튜닝하면 이 지식이 로봇 행동으로 연결됨

#### RT-2의 아키텍처

```
RT-2 아키텍처:

사전학습된 VLM 기반:
  - PaLI-X (55B) 또는 PaLM-E (12B)
  - 이미 인터넷 규모 이미지+텍스트로 학습됨

입력:
  카메라 이미지 + 자연어 명령
  "What action should the robot take to pick up the apple?"

출력 (핵심!):
  행동을 텍스트 토큰으로 표현!
  예: "1 128 91 241 5 101 127"
       ↑   ↑   ↑   ↑  ↑  ↑   ↑
      terminate x   y   z  rx ry  rz/gripper
       (0=계속, 1=종료)

  각 값: 0~255 범위의 정수 (256 bins)
  이 정수들을 텍스트 토큰 어휘에 추가
```

#### 행동을 텍스트 토큰으로: 핵심 아이디어

```
기존 접근:
  언어 모델 → 텍스트 출력
  "The robot should move left and close the gripper"
  → 텍스트를 다시 해석해서 행동으로 변환 (느리고 부정확)

RT-2의 접근:
  언어 모델 → 행동 토큰 직접 출력
  토큰 ID 128 = x축 128/256 위치
  토큰 ID 91 = y축 91/256 위치
  → 언어 모델이 "단어"를 생성하듯 "행동"을 생성

왜 이것이 작동하는가:
  Transformer는 "다음 토큰 예측"을 한다
  다음 토큰이 영어 단어이든, 로봇 행동 값이든 동일한 메커니즘
  사전학습된 패턴 인식 능력이 행동 예측에도 활용됨
```

#### Chain-of-Thought for Robotics

RT-2는 로봇 영역에서 Chain-of-Thought(CoT) 추론도 시도했다:

```
일반 CoT (언어 모델):
  Q: "17 * 23 = ?"
  A: "17 * 20 = 340, 17 * 3 = 51, 340 + 51 = 391"

RT-2 CoT (로봇):
  명령: "탄산음료를 골라 집어라"
  추론: "I see a Coca-Cola can. Coca-Cola is a carbonated drink."
  행동: "1 142 89 200 12 98 140" (콜라 캔을 향한 행동)

효과:
  - 중간 추론을 명시적으로 생성
  - 의미적 추론이 필요한 task에서 성능 향상
  - "더 작은 물체를 집어라" → 크기 비교 추론 가능
```

### 3. RT-2의 Emergent Capabilities (창발적 능력)

RT-2에서 가장 놀라운 결과는 학습하지 않은 능력의 출현이다:

```
학습 데이터에 없는 능력들:

1. 물체 범주 이해
   명령: "음료를 집어라"
   → 학습에 "음료" 카테고리가 없었지만, 콜라/주스를 정확히 집음
   → VLM의 언어 지식에서 전이됨

2. 상대적 비교
   명령: "가장 작은 물체를 옮겨라"
   → 크기 비교 능력이 VLM에서 전이됨

3. 아이콘/기호 이해
   명령: "이 그림이 나타내는 나라의 국기 색상 물체를 집어라"
   → 시각적 추론 + 세계 지식 결합

4. 새로운 물체 일반화
   학습에 없던 물체도 자연어 설명으로 조작 가능
   → "공룡 장난감을 집어라" (학습 데이터에 공룡 없음)

이 모든 것은 VLM의 사전학습 지식에서 나옴!
→ 로봇 데이터로는 절대 학습할 수 없는 능력들
```

### 4. RT-1 vs RT-2 비교

```
              RT-1                    RT-2
─────────────────────────────────────────────────────
기반 모델     처음부터 학습            사전학습 VLM 파인튜닝
파라미터      ~35M                    55B (PaLI-X)
학습 데이터   로봇 데이터만            인터넷 + 로봇 데이터
일반화        학습한 물체만            새로운 물체도 가능
추론 능력     없음                    Chain-of-Thought 가능
의미 이해     제한적                  VLM 수준의 이해
행동 표현     이산 토큰               텍스트 토큰 (동일 포맷)
추론 속도     빠름 (~3Hz)            느림 (~1-3Hz)
배포 용이성   엣지 가능               대형 서버 필요

핵심 교훈:
  RT-1 → "데이터 규모를 키우면 범용 로봇이 가능하다"
  RT-2 → "사전학습 지식을 전이하면 데이터 효율이 극적으로 향상된다"
  이 두 교훈이 이후 모든 VLA 연구의 기초가 됨
```

### 5. RT-2에서 이후 VLA로의 영향

```
RT-2의 설계 선택이 이후 모델에 미친 영향:

"행동을 토큰으로" 아이디어:
  RT-2    → 256 bins, 텍스트 토큰
  OpenVLA → 256 bins, 확장 어휘 (동일 아이디어)
  FAST    → DCT 압축 토큰 (발전된 형태)

"VLM 파인튜닝" 아이디어:
  RT-2    → PaLI-X/PaLM-E 파인튜닝
  OpenVLA → Prismatic VLM → Llama 2 파인튜닝
  pi-0    → PaLiGemma 파인튜닝
  Helix   → 자체 VLM 파인튜닝

"대규모 데이터" 아이디어:
  RT-1/RT-2 → 내부 데이터만
  Open X-Embodiment → 22개 로봇, 1M+ 에피소드 (공유 데이터셋)
  LeRobot → 커뮤니티 기반 데이터 수집

RT-2의 한계가 후속 연구를 촉발:
  - 너무 큼 (55B) → SmolVLA (450M)
  - 비공개 → OpenVLA (오픈소스)
  - 이산 토큰만 → pi-0 (연속 Flow Matching)
  - 느린 제어 → Helix (200Hz System 1)
```

---

## 연습 주제 (코드 없이 생각해보기)

1. **RT-1의 한계 분석**: RT-1이 13만 에피소드를 학습했음에도 "탄산음료를 집어라"라는 명령을 처리할 수 없는 이유를 설명하라. 데이터를 10배로 늘리면 해결될까? 왜/왜 아닌가?

2. **행동 이산화 이해**: 로봇 팔의 x축 이동 범위가 -0.5m ~ +0.5m일 때, 256 bins으로 이산화하면 한 bin의 해상도는 얼마인가? 이 해상도가 정밀 조작(예: 바늘 집기)에 충분한지 생각해보라.

3. **VLM 지식 전이**: "빨간 컵 옆에 있는 초록 물체를 집어라"라는 명령을 RT-1과 RT-2가 각각 어떻게 처리할지 비교하라. RT-2가 이 명령을 더 잘 처리할 수 있는 구체적 이유는 무엇인가?

4. **Chain-of-Thought의 비용**: RT-2가 CoT 추론을 수행하면 추가 토큰을 생성해야 한다. 이것이 로봇 제어 주파수(Hz)에 미치는 영향을 설명하라. 실시간 제어에서 이것이 왜 문제가 될 수 있는가?

5. **창발 능력의 본질**: RT-2의 "창발적 능력"은 사실 VLM에 이미 존재하던 능력이다. 로봇 파인튜닝이 이 능력을 "파괴"하지 않고 "보존"하면서 동시에 행동 생성 능력을 추가한 이유를 생각해보라. (힌트: LoRA와 같은 파인튜닝 기법의 역할)

6. **규모 문제**: RT-2의 55B 파라미터 모델을 로봇에 직접 탑재할 수 있을까? 필요한 GPU 메모리와 추론 지연시간을 대략적으로 추정하라. 이것이 후속 연구(SmolVLA, Dual-System)에 어떤 동기를 부여했는가?

---
