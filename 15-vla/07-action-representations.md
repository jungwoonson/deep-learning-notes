# Action Representations (행동 표현 방법론)

## VLA와의 연결

**행동 표현(Action Representation)은 VLA 설계에서 가장 중요한 선택 중 하나이다.** "로봇이 어떻게 움직여야 하는가"를 모델이 어떤 형태로 출력하는지에 따라 성능, 속도, 정밀도가 크게 달라진다. RT-2와 OpenVLA는 행동을 이산 토큰(discrete tokens)으로, pi-0과 SmolVLA는 연속 분포(continuous distribution)로 표현한다. 이 노트는 각 방법의 원리, 장단점, 그리고 2025년 최신 방법론(FAST, FASTer, VQ-VLA)까지 종합적으로 비교한다.

---

## 핵심 개념

### 1. 로봇 행동이란 무엇인가

```
로봇 팔의 행동 (일반적인 7-DoF):

한 타임스텝의 행동:
  [delta_x, delta_y, delta_z, delta_rx, delta_ry, delta_rz, gripper]
   위치 변화 (3)         회전 변화 (3)              개폐 (1)

  delta_x:    엔드이펙터의 x축 이동량 (예: +0.02m = 2cm 앞으로)
  delta_y:    엔드이펙터의 y축 이동량
  delta_z:    엔드이펙터의 z축 이동량
  delta_rx:   x축 회전량 (roll)
  delta_ry:   y축 회전량 (pitch)
  delta_rz:   z축 회전량 (yaw)
  gripper:    그리퍼 상태 (0.0 = 닫힘, 1.0 = 열림)

각 값은 연속적 실수(float):
  예: [0.023, -0.011, 0.005, 0.001, -0.003, 0.002, 0.8]

문제: 이 연속값을 신경망이 어떻게 출력하는가?
  → 이것이 "Action Representation" 문제
```

### 2. 방법 1: 이산 토큰 (Discrete Tokens)

RT-2와 OpenVLA가 사용하는 방식:

```
이산화(Discretization) 과정:

1. 각 행동 차원의 범위를 설정
   예: delta_x 범위 = [-0.05, +0.05] (미터)

2. 범위를 N개 bin으로 균등 분할
   N = 256 (OpenVLA)
   bin 크기 = 0.1 / 256 ≈ 0.00039m ≈ 0.4mm

3. 연속값을 가장 가까운 bin에 매핑
   delta_x = 0.023 → bin 178 (= round((0.023 + 0.05) / 0.1 * 255))

4. bin 인덱스를 토큰으로 출력
   모델 출력: [178, 114, 141, 130, 124, 131, 204]
               x    y    z    rx   ry   rz   gripper

장점:
  - LLM의 다음 토큰 예측 프레임워크를 그대로 사용 가능
  - Cross-Entropy Loss로 학습 (검증된 방법)
  - 구현이 단순함
  - 확률 분포를 출력 → 불확실성 표현 가능

단점:
  - 양자화 오류 (0.4mm 해상도의 한계)
  - 차원 간 독립 가정 (x, y를 독립적으로 이산화)
  - 다차원 상관관계를 포착하기 어려움
  - bin 수 증가 시 어휘 크기 폭발
```

### 3. 방법 2: FAST 토크나이저 (Compressed Tokens)

pi-0-FAST에서 도입된 방식:

```
FAST (Fast Action Tokenization):

핵심 아이디어:
  행동 시퀀스를 "주파수 도메인"으로 변환하여 압축

단계별 과정:

1. Action Chunk 수집
   10 타임스텝의 행동 시퀀스:
   [[a1_t1, a2_t1, ..., a7_t1],   ← 타임스텝 1
    [a1_t2, a2_t2, ..., a7_t2],   ← 타임스텝 2
    ...
    [a1_t10, a2_t10, ..., a7_t10]] ← 타임스텝 10

   크기: 10 x 7 = 70 값

2. DCT 변환 (Discrete Cosine Transform)
   각 행동 차원에 대해 시간축으로 DCT 적용
   → 저주파 성분 (부드러운 동작의 큰 흐름)
   → 고주파 성분 (미세한 떨림, 노이즈)

3. 고주파 성분 절단 (Truncation)
   상위 K개 DCT 계수만 유지
   → 70 값 → 약 15-20 값으로 압축 (3-5배 압축)

4. 양자화 + 토큰화
   남은 DCT 계수를 이산 토큰으로 변환

왜 DCT인가:
  - JPEG 이미지 압축의 핵심 기술
  - 로봇 행동의 "부드러운 궤적"은 저주파 성분이 지배적
  - 고주파 = 노이즈 또는 미세 진동 → 제거해도 행동 품질 유지
  - 압축률이 높으면서 정보 손실이 작음

비유:
  음악을 MP3로 압축할 때:
  - 인간이 못 듣는 고주파를 제거
  - 음질은 거의 같지만 파일 크기는 1/10

  로봇 행동을 FAST로 압축할 때:
  - 행동에 기여하지 않는 고주파를 제거
  - 행동 품질은 거의 같지만 토큰 수는 1/4
```

### 4. 방법 3: 연속 Flow Matching (Continuous Distribution)

pi-0, SmolVLA가 사용하는 방식:

```
Flow Matching 기반 행동 생성:

핵심 아이디어:
  행동을 "이산 선택"이 아닌 "연속 분포에서 샘플링"으로 생성

과정:
  1. 가우시안 노이즈 z ~ N(0, I) 에서 시작
     z는 행동과 같은 차원 (예: 7-DoF → 7차원 벡터)

  2. 학습된 벡터장(vector field)을 따라 이동
     z_0 = z (노이즈)
     z_1 = z_0 + v(z_0, t=0, condition) * dt
     z_2 = z_1 + v(z_1, t=dt, condition) * dt
     ...
     z_N = 최종 행동 (연속값)

  3. z_N이 직접 로봇에 전달됨 (양자화 없음!)

  여기서 condition = VLM의 시각-언어 표현

장점:
  - 양자화 오류 없음 (연속값 직접 출력)
  - 다차원 상관관계 자연스럽게 포착
    → x와 y의 동시 변화(대각선 이동)를 자연스럽게 생성
  - 다봉 분포(multimodal distribution) 표현 가능
    → "왼쪽으로 돌아가거나 오른쪽으로 돌아가거나" 두 옵션
  - Action Chunk (여러 타임스텝) 동시 생성에 적합

단점:
  - 여러 Denoising Step 필요 (추론 시간 증가)
  - LLM 프레임워크와 직접 호환되지 않음
  - 학습이 이산 토큰보다 복잡할 수 있음
  - 불확실성 해석이 이산 분포보다 덜 직관적
```

### 5. 비교표

```
                이산 토큰         FAST 토큰          Flow Matching
                (RT-2, OpenVLA)   (pi-0-FAST)        (pi-0, SmolVLA)
─────────────────────────────────────────────────────────────────────
출력 형태       정수 토큰          압축 토큰          연속 벡터
양자화          있음 (256 bins)   있음 (DCT 후 양자화) 없음
차원 간 상관    독립 가정          DCT로 부분 포착     완전 포착
토큰 수/스텝    7 (7-DoF)        ~2-3 (압축)         N/A (벡터)
토큰 수/chunk   70 (10스텝)      ~15-20 (압축)       N/A
학습 Loss       Cross-Entropy     Cross-Entropy      Flow Matching Loss
LLM 호환        완벽              완벽                별도 Head 필요
정밀도          ~0.4mm            ~0.5mm              이론적 무한
다봉 분포       어려움             어려움               자연스러움
학습 속도       기준               5배 빠름            기준
추론 스텝       1 (argmax)        1 (argmax)          10+ (denoising)
구현 복잡도     낮음               중간                높음
```

### 6. FASTer (2025.12)

```
FASTer: FAST의 진화

FAST의 한계:
  - 고정된 DCT 기저(basis)를 사용
  - 모든 로봇/task에 동일한 압축 적용
  - 특정 로봇의 행동 특성을 반영하지 못함

FASTer의 개선:
  1. 학습 가능한 압축 기저
     DCT 대신 데이터에서 최적 기저를 학습
     → 특정 로봇에 최적화된 압축

  2. 적응적 압축률
     task 복잡도에 따라 토큰 수 자동 조절
     단순 task → 더 적은 토큰
     복잡 task → 더 많은 토큰

  3. 더 나은 재구성 품질
     학습된 기저가 DCT보다 행동 분포에 적합
     → 같은 압축률에서 더 높은 품질

결과:
  FAST 대비 추가 30-50% 토큰 수 감소
  동일 또는 더 나은 행동 품질
```

### 7. VQ-VLA (Vector Quantized VLA)

```
VQ-VLA: VQ-VAE를 행동에 적용

VQ-VAE 복습 (Part 14 연결):
  연속 데이터 → Encoder → 이산 코드북(codebook) → Decoder → 재구성
  이미지 생성에서 검증된 기법

VQ-VLA의 접근:
  1. 행동 VQ-VAE 학습
     연속 행동 시퀀스 → Encoder → 이산 코드 → Decoder → 행동 재구성
     코드북: K개의 행동 "원형(prototype)"

  2. VLA 학습
     이미지 + 언어 → VLM → 이산 코드 예측 (코드북에서 선택)
     → 예측된 코드 → VQ-VAE Decoder → 연속 행동 출력

장점:
  - 이산 토큰의 LLM 호환성 + 연속 행동의 정밀도
  - 코드북이 의미있는 행동 패턴을 학습
    예: 코드 42 = "위로 들어올리기", 코드 107 = "왼쪽으로 밀기"
  - 차원 간 상관관계를 Encoder가 학습

한계:
  - 추가 VQ-VAE 학습이 필요 (2단계 학습)
  - 코드북 크기 선택이 중요 (너무 작으면 표현력 부족)
  - 아직 초기 연구 단계
```

### 8. 어떤 방법을 선택해야 하는가

```
상황별 권장 방법:

"빠르게 프로토타이핑하고 싶다"
  → 이산 토큰 (256 bins)
  → 구현이 가장 단순, LLM 프레임워크 그대로 사용

"기존 LLM 인프라를 최대 활용하고 싶다"
  → FAST 토크나이저
  → Autoregressive 생성 + 효율적 압축

"최고 품질의 행동이 필요하다"
  → Flow Matching
  → 연속 출력, 다봉 분포, 부드러운 궤적

"작은 모델로 좋은 성능이 필요하다"
  → SmolVLA (Flow Matching) 또는 FAST
  → 압축으로 효율성 확보

"연구/실험 목적이다"
  → 여러 방법을 비교 실험
  → 각 방법의 특성을 직접 확인

2026년 트렌드:
  - Flow Matching이 주류로 자리잡는 추세
  - FAST 계열이 Autoregressive 진영에서 표준화
  - 하이브리드 접근 (VQ-VLA 등)이 새롭게 부상
  - "하나의 정답"은 아직 없음 → 활발한 연구 중
```

### 9. Action Chunking: 시간 축 표현

```
행동 표현의 또 다른 차원: 시간

단일 스텝 예측:
  모델이 한 번에 1개 타임스텝의 행동만 출력
  → 매 타임스텝마다 추론 필요 (비효율)
  → 시간적 일관성(temporal consistency) 부족

Action Chunking:
  모델이 한 번에 여러 타임스텝의 행동을 출력
  예: 16 스텝 × 7 DoF = 112 값의 "행동 덩어리(chunk)"

  장점:
    1. 추론 빈도 감소 (16배 덜 자주 추론)
    2. 부드러운 궤적 (시간적 일관성 보장)
    3. 비동기 실행 가능

  현재 chunk를 실행하면서 다음 chunk를 미리 계산:
    시간:  |--- chunk 1 실행 ---|--- chunk 2 실행 ---|
    추론:  |계산|              |계산|              |계산|
    → 끊김 없는 연속 동작

  대부분의 최신 VLA가 Action Chunking을 사용:
    pi-0: 50 스텝 chunk
    SmolVLA: 16 스텝 chunk
    GR00T N1: 가변 chunk
```

---

## 연습 주제 (코드 없이 생각해보기)

1. **양자화 오류 누적**: 7-DoF 로봇에서 256 bins를 사용할 때, 최악의 경우 각 차원에서 bin 크기의 절반만큼 오류가 발생한다. 100 타임스텝의 궤적에서 이 오류가 누적되면 엔드이펙터 위치가 얼마나 벗어날 수 있는가? (힌트: 오류는 독립적이므로 sqrt(N)에 비례)

2. **DCT의 직관**: 사인파(sin wave)와 계단 함수(step function)를 각각 DCT로 변환하면 어떤 차이가 있는가? 로봇 행동 중 "부드러운 이동"과 "급격한 그리퍼 닫기"는 각각 어떤 주파수 특성을 가지는가?

3. **다봉 분포 문제**: 컵이 테이블 양쪽에 하나씩 있을 때 "컵을 집어라"라는 명령에 대해, 이산 토큰(argmax)과 Flow Matching(sampling)의 출력이 어떻게 다를지 비교하라. 이산 방식이 "두 컵 사이의 빈 공간"을 향해 행동할 위험이 있는 이유는?

4. **FAST vs FASTer**: FASTer가 DCT 대신 "학습된 기저"를 사용하는 것이 왜 더 나을 수 있는지, 구체적 예를 들어 설명하라. (힌트: 휴머노이드의 보행 패턴과 로봇 팔의 집기 패턴은 매우 다른 주파수 특성을 가진다)

5. **VQ-VLA의 코드북**: VQ-VLA에서 코드북 크기가 64, 256, 1024일 때 각각의 장단점은? 코드북이 너무 작으면 어떤 문제가, 너무 크면 어떤 문제가 발생하는가?

6. **Action Chunk 크기 선택**: chunk 크기가 4, 16, 64 스텝일 때 각각의 장단점을 분석하라. 작은 chunk는 반응성이 좋지만, 큰 chunk는 효율적이다. "컵을 들고 있다가 미끄러지기 시작할 때" 어떤 chunk 크기가 유리한가?

---

## 다음 노트

**다음**: [Datasets & Open X-Embodiment](./08-datasets-open-x.md) - VLA의 성능은 결국 데이터에 의해 결정된다. Open X-Embodiment(22개 로봇, 1M+ 에피소드), RLDS 형식, LeRobot 커뮤니티 데이터셋, 그리고 자체 데이터를 VLA 학습에 사용하기 위한 준비 과정.
