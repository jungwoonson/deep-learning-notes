# 선형 회귀 (Linear Regression)

## 왜 알아야 하는가 (Why This Matters for VLA)

선형 회귀는 가장 단순한 형태의 "뉴럴 네트워크"이다. 활성화 함수(activation function) 없이 뉴런 하나만 있는 네트워크가 바로 선형 회귀이다. VLA 모델의 거대한 신경망도 결국 이 단순한 연산의 반복과 조합으로 이루어져 있다.

VLA와의 연결 고리:
- VLA의 Action 파트에서 로봇 팔의 **관절 각도나 위치(연속값)를 예측**하는 것은 본질적으로 회귀 문제이다
- 신경망의 모든 레이어는 $y = Wx + b$ 형태의 선형 변환을 기본으로 한다
- **손실 함수(Loss Function)** 와 **경사 하강법(Gradient Descent)** 이라는 학습의 핵심 메커니즘을 선형 회귀에서 처음 만나게 된다
- 여기서 배우는 원리가 CNN, Transformer, 그리고 VLA 전체로 확장된다

---

## 핵심 개념 (Core Concepts)

### 1. 선형 회귀란? (What is Linear Regression?)

입력(X)과 출력(Y) 사이의 **선형 관계**를 찾는 것이다.

**단순 선형 회귀 (Simple Linear Regression)**:

$$y = wx + b$$

- $y$: 예측값 (prediction)
- $x$: 입력 특성 (input feature)
- $w$: 가중치 (weight) - 기울기
- $b$: 편향 (bias) - y절편

**다중 선형 회귀 (Multiple Linear Regression)**:

$$y = w_1 x_1 + w_2 x_2 + \cdots + w_n x_n + b = W^T X + b$$

예: 집값 $= w_1 \times \text{면적} + w_2 \times \text{방수} + w_3 \times \text{역거리} + b$

**뉴럴 네트워크와의 관계**:
```
[입력] → [뉴런: y = Wx + b] → [출력]

활성화 함수가 없는 뉴런 하나 = 선형 회귀
활성화 함수를 추가하면   = 로지스틱 회귀 / 퍼셉트론
이것을 여러 층 쌓으면    = 딥 뉴럴 네트워크
```

### 2. 손실 함수: MSE (Loss Function: Mean Squared Error)

모델의 예측이 **얼마나 틀렸는지** 수치로 표현하는 함수이다.

$$\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

- $y_i$: 실제값 (ground truth)
- $\hat{y}_i$: 예측값 (prediction)
- $n$: 데이터 개수

**왜 제곱인가?**
- 오차의 부호(+/-)를 없앤다 (양수든 음수든 "틀린 정도"만 측정)
- 큰 오차에 더 큰 페널티를 준다 (오차 2 → 페널티 4, 오차 3 → 페널티 9)
- 미분이 깔끔하게 나온다 (경사 하강법에 유리)

**직관적 이해**:
```
실제 집값:  [3억, 5억, 4억]
예측 집값:  [3.5억, 4억, 4.2억]
오차:       [0.5, -1.0, 0.2]
오차 제곱:  [0.25, 1.0, 0.04]
MSE = (0.25 + 1.0 + 0.04) / 3 = 0.43
```

### 3. 경사 하강법 (Gradient Descent)

손실 함수를 **최소화**하는 파라미터($w$, $b$)를 찾는 방법이다.

**산 비유**: 안개 낀 산에서 가장 낮은 곳(골짜기)을 찾고 싶다.
- 현재 위치에서 **가장 가파르게 내려가는 방향**을 느낀다 (= 기울기/gradient 계산)
- 그 방향으로 **한 걸음** 내려간다 (= 파라미터 업데이트)
- 이것을 반복하면 결국 골짜기(최솟값)에 도달한다

$$\begin{aligned}
w &\leftarrow w - \eta \cdot \frac{\partial L}{\partial w} \\
b &\leftarrow b - \eta \cdot \frac{\partial L}{\partial b}
\end{aligned}$$

**학습률 (Learning Rate, $\eta$)**:
- 한 걸음의 크기를 결정한다
- **너무 크면**: 골짜기를 넘어가 발산한다 (overshooting)
- **너무 작으면**: 학습이 매우 느리다
- **적절한 값**: 보통 0.001 ~ 0.01에서 시작하여 조정한다

```
lr이 너무 클 때:     lr이 적절할 때:     lr이 너무 작을 때:
    /\                  \                   \
   /  \  /\              \                   \
  /    \/  \              \                    \
 /          \              \.                    \..........
 (발산!)            (수렴!)             (매우 느림)
```

### 4. MSE에 대한 경사 하강법 유도

선형 회귀 + MSE 조합에서의 gradient를 직관적으로 이해해보자:

$$\hat{y} = wx + b$$

$$L = \frac{1}{n} \sum (y - \hat{y})^2$$

$$\frac{\partial L}{\partial w} = \frac{1}{n} \sum 2(\hat{y} - y) \cdot x$$

$$\frac{\partial L}{\partial b} = \frac{1}{n} \sum 2(\hat{y} - y)$$

직관: "예측값이 실제값보다 크면 → $w$와 $b$를 줄여라", "예측값이 실제값보다 작으면 → $w$와 $b$를 키워라"

### 5. 특성 스케일링 (Feature Scaling)

서로 다른 범위의 특성들을 **비슷한 스케일**로 맞춰주는 것이다.

**왜 필요한가?**
```
면적: 10 ~ 300 (m^2)
방 개수: 1 ~ 10

→ 면적의 값이 훨씬 크므로, 면적에 대한 gradient도 훨씬 커짐
→ 경사 하강법이 비효율적으로 지그재그 이동함
→ 학습이 느려지거나 불안정해짐
```

**주요 방법**:

**Min-Max Scaling (정규화)**:

$$x_{\text{scaled}} = \frac{x - x_{\min}}{x_{\max} - x_{\min}}$$

→ 모든 값을 0~1 사이로 변환

**Standardization (표준화)**:

$$x_{\text{scaled}} = \frac{x - \mu}{\sigma}$$

→ 평균 0, 표준편차 1로 변환. 딥러닝에서 가장 많이 사용.

**주의사항**:
- 스케일링 기준(mean, std 등)은 반드시 **훈련 세트에서만** 계산한다
- 검증/테스트 세트에는 훈련 세트의 기준을 그대로 적용한다
- 그렇지 않으면 미래 데이터 정보가 유출(data leakage)된다

### 6. 정규 방정식 vs. 경사 하강법 (Normal Equation vs. Gradient Descent)

선형 회귀는 해석적 해(closed-form solution)가 존재한다:

$$w = (X^T X)^{-1} X^T y$$

- 장점: 한 번에 정확한 해를 구함, 학습률 불필요
- 단점: 특성이 많으면($n > 10{,}000$) 행렬 역행렬 계산이 매우 느림

경사 하강법:
- 장점: 특성이 매우 많아도 작동, 다른 모든 모델에도 적용 가능
- 단점: 학습률 설정 필요, 여러 번 반복해야 함
- → 딥러닝에서는 경사 하강법만 사용한다 (정규 방정식은 선형 회귀 전용)

---

## 연습 주제 (Practice Topics)

스스로 생각해보고 답을 정리해 보자 (코드 작성 불필요):

1. **직관 테스트**: 데이터 포인트가 $(1, 2), (2, 4), (3, 6)$일 때, 최적의 $w$와 $b$는 무엇인가? 손으로 계산해 보자.

2. **MSE 계산**: 실제값이 $[10, 20, 30]$이고 예측값이 $[12, 18, 25]$일 때, MSE를 손으로 계산하라.

3. **학습률 실험 설계**: 학습률이 $0.1$, $0.01$, $0.001$일 때 각각 어떤 일이 벌어질지 예상해 보고, 어떤 기준으로 적절한 학습률을 판단할 수 있는지 생각해 보자.

4. **스케일링 필요성**: 특성 A의 범위가 $[0, 1]$이고 특성 B의 범위가 $[0, 1000000]$일 때, 스케일링 없이 경사 하강법을 수행하면 어떤 문제가 생기는가?

5. **VLA 연결**: 로봇 팔이 물체까지의 거리($x_1$), 물체의 무게($x_2$), 목표 높이($x_3$)를 입력받아 필요한 힘($y$)을 예측하는 문제를 선형 회귀로 모델링한다면, 수식은 어떻게 되는가?

---

## 다음 노트 (Next Note)

선형 회귀로 연속값을 예측하는 방법을 배웠다. 이제 "이것은 A인가 B인가?"를 판단하는 **분류 문제**로 넘어가자.

**다음**: [로지스틱 회귀 (Logistic Regression)](./03-logistic-regression.md) - 선형 회귀에 시그모이드 함수를 추가하면 분류가 가능해진다.
