# 로지스틱 회귀 (Logistic Regression)

## 왜 알아야 하는가 (Why This Matters for VLA)

이름에 "회귀"가 들어가지만, 로지스틱 회귀는 **분류(Classification)** 모델이다. 선형 회귀의 출력을 0과 1 사이의 **확률**로 변환하여 "이것은 A인가 B인가?"를 판단한다.

VLA와의 연결 고리:
- VLA의 Vision 파트에서 "이 물체가 컵인가? 접시인가? 포크인가?"를 판단하는 것이 **다중 분류(Multi-class Classification)** 이다
- 이때 사용되는 **softmax 함수**가 바로 로지스틱 회귀의 확장이다
- VLA의 Language 파트에서 다음 단어를 예측할 때, 전체 어휘 중 하나를 고르는 것도 분류 문제이다
- **시그모이드(sigmoid)** 와 **소프트맥스(softmax)** 는 Transformer를 포함한 거의 모든 딥러닝 모델에서 사용된다
- 여기서 배우는 **교차 엔트로피(Cross-Entropy)** 손실 함수는 딥러닝 분류 문제의 표준 손실 함수이다

---

## 핵심 개념 (Core Concepts)

### 1. 왜 선형 회귀로 분류를 하면 안 되는가?

```
문제: 이메일이 스팸(1)인지 정상(0)인지 분류하기

선형 회귀를 그대로 쓰면:
  y = wx + b  →  출력이 -2.3, 0.7, 1.5, 3.2 ... (범위 제한 없음)

문제점:
  - 출력이 0~1 범위를 벗어남 → 확률로 해석할 수 없음
  - 극단적인 데이터 하나가 전체 결정 경계를 왜곡함
```

**해결**: 선형 회귀의 출력을 0~1 사이로 "압축"하는 함수를 추가하자 → **시그모이드 함수**

### 2. 시그모이드 함수 (Sigmoid Function)

$$\sigma(z) = \frac{1}{1 + e^{-z}}, \quad \text{여기서 } z = wx + b$$

**특성**:
- $z$가 매우 큰 양수 → $\sigma(z) \approx 1$ (거의 확실히 양성)
- $z = 0$ → $\sigma(z) = 0.5$ (반반)
- $z$가 매우 큰 음수 → $\sigma(z) \approx 0$ (거의 확실히 음성)

```
그래프:
  1.0 |          ___________
      |         /
  0.5 |--------/------------ (z = 0일 때 0.5)
      |       /
  0.0 |______/
      -∞     0     +∞
```

**로지스틱 회귀의 전체 흐름**:
```
입력(x) → 선형 변환(z = wx + b) → 시그모이드(σ(z)) → 확률(0~1) → 판정(0 or 1)

예측 확률이 0.5 이상 → 클래스 1 (스팸)
예측 확률이 0.5 미만 → 클래스 0 (정상)
```

### 3. 이진 교차 엔트로피 (Binary Cross-Entropy Loss)

분류 문제에는 MSE 대신 **교차 엔트로피**를 사용한다.

$$\text{BCE} = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]$$

- $y$: 실제 레이블 (0 또는 1)
- $\hat{y}$: 모델이 예측한 확률 (0~1 사이)

**직관적 이해**:

```
Case 1: 실제로 스팸(y=1)인데, 모델이 0.9라고 예측
  → -[1*log(0.9)] = 0.105 (낮은 손실, 잘 예측!)

Case 2: 실제로 스팸(y=1)인데, 모델이 0.1이라고 예측
  → -[1*log(0.1)] = 2.302 (높은 손실, 크게 틀림!)

Case 3: 실제로 정상(y=0)인데, 모델이 0.9라고 예측 (스팸이라 잘못 판단)
  → -[log(1-0.9)] = -log(0.1) = 2.302 (높은 손실!)
```

**왜 MSE가 아니라 교차 엔트로피인가?**
- MSE + 시그모이드 조합은 gradient가 매우 작아지는 구간이 있다 (vanishing gradient)
- 교차 엔트로피는 틀릴수록 gradient가 커져서 빠르게 교정된다
- 확률 분포 간의 차이를 측정하는 정보 이론(information theory)적 근거가 있다

### 4. 다중 분류와 소프트맥스 (Multi-class Classification & Softmax)

클래스가 3개 이상일 때는 시그모이드 대신 **소프트맥스(softmax)** 를 사용한다.

$$\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$

$K$개의 클래스에 대한 확률을 출력하며, 전체 합이 1이 된다.

**예시: 숫자 0, 1, 2 분류**

```
선형 출력 z = [2.0, 1.0, 0.1]

e^z = [e^2.0, e^1.0, e^0.1] = [7.39, 2.72, 1.11]
합계 = 7.39 + 2.72 + 1.11 = 11.22

softmax = [7.39/11.22, 2.72/11.22, 1.11/11.22]
        = [0.659, 0.242, 0.099]

해석: 클래스 0일 확률 65.9%, 클래스 1일 확률 24.2%, 클래스 2일 확률 9.9%
예측: 클래스 0 (가장 높은 확률)
```

**다중 분류의 손실 함수: Categorical Cross-Entropy**

$$\text{CCE} = -\sum_{k=1}^{K} y_k \log(\hat{y}_k)$$

$y_k$: one-hot 벡터 (정답 클래스만 1, 나머지 0)

```
예: 정답이 클래스 0이면 y = [1, 0, 0]
    예측이 [0.659, 0.242, 0.099]이면
    CCE = -[1*log(0.659) + 0*log(0.242) + 0*log(0.099)]
        = -log(0.659) = 0.417
```

### 5. 결정 경계 (Decision Boundary)

로지스틱 회귀가 클래스를 나누는 **경계선**이다.

결정 경계: $\sigma(wx + b) = 0.5$ → $wx + b = 0$

2차원 입력의 경우: $w_1 x_1 + w_2 x_2 + b = 0$ → 직선(linear boundary)

```
특성(x2)
  |     ○ ○ ○
  |   ○ ○ /
  |  ○ ○ / × ×
  |   ○ / × × ×
  |    / × × ×
  |   / × × ×
  +------------- 특성(x1)
     (/ = 결정 경계)
```

**한계**:
- 로지스틱 회귀의 결정 경계는 항상 **직선(또는 초평면)** 이다
- 데이터가 직선으로 나눌 수 없는 경우(예: XOR 문제) 분류가 불가능하다
- 이 한계를 극복하기 위해 **뉴럴 네트워크**(다층 퍼셉트론)가 등장한다

```
선형 분류 가능:          선형 분류 불가능(XOR):
○ ○ | × ×               ○ ×
○ ○ | × ×               × ○
  (직선으로 분리 가능)     (직선으로 분리 불가)

→ 뉴럴 네트워크는 비선형 결정 경계를 학습할 수 있다
```

### 6. 전체 흐름 요약 (Summary Pipeline)

```
[이진 분류]
입력 → z = Wx + b → sigmoid(z) → 확률 → BCE Loss → Gradient Descent

[다중 분류]
입력 → z = Wx + b → softmax(z) → 확률 벡터 → CCE Loss → Gradient Descent
       (K개 출력)   (K개 확률)
```

---

## 연습 주제 (Practice Topics)

스스로 생각해보고 답을 정리해 보자 (코드 작성 불필요):

1. **시그모이드 계산**: $z = 0, z = 2, z = -2$일 때 $\sigma(z)$를 각각 계산하라. 계산기를 사용해도 좋다.

2. **BCE 손실 비교**: 실제 레이블이 1일 때, 예측 확률이 0.99인 경우와 0.51인 경우의 BCE를 각각 계산하고 비교하라.

3. **소프트맥스 계산**: 선형 출력이 $[3.0, 1.0, 0.5]$일 때, 소프트맥스 확률을 손으로 계산하라.

4. **결정 경계 그리기**: $w_1 = 2, w_2 = -1, b = 1$일 때, 결정 경계($w_1 x_1 + w_2 x_2 + b = 0$)를 $x_1$-$x_2$ 좌표계에 그려보라.

5. **VLA 분류 시나리오**: VLA 로봇이 앞에 있는 물체를 10가지 종류 중 하나로 분류해야 한다. 이때 출력층은 어떤 구조여야 하는가? (뉴런 수, 활성화 함수, 손실 함수)

---
