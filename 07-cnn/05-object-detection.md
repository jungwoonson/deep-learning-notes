# 객체 탐지 (Object Detection)

## 왜 필요한가

VLA 모델의 로봇은 단순히 "이미지에 컵이 있다"를 넘어서, **"컵이 이미지의 어디에 있는가"** 를 알아야 한다. 물체를 집으려면 위치를 알아야 하고, 장애물을 피하려면 장애물의 영역을 알아야 한다. 객체 탐지는 이미지 분류를 공간적 이해로 확장하는 핵심 기술이며, 로봇의 시각적 인식(visual perception) 파이프라인의 중요한 구성 요소다.

---

## 핵심 개념

### 1. Detection vs Classification

이미지 분류와 객체 탐지는 근본적으로 다른 문제다.

- **Classification (분류)**: "이 이미지에 무엇이 있는가?" → 하나의 라벨 출력
- **Object Detection (객체 탐지)**: "이 이미지에 무엇이, 어디에, 몇 개 있는가?" → 여러 개의 라벨 + 위치 출력
- 탐지는 분류보다 훨씬 어려운 문제다:
  - 물체의 수가 이미지마다 다르다 (가변 출력)
  - 물체의 크기가 제각각이다
  - 물체가 서로 겹칠 수 있다
  - 위치를 정확히 특정해야 한다

### 2. Bounding Box

물체의 위치를 표현하는 가장 기본적인 방법이 **바운딩 박스(bounding box)** 다.

- 물체를 감싸는 직사각형으로, 보통 네 가지 값으로 표현한다:
  - **(x, y, w, h)**: 중심 좌표(x, y)와 너비(w), 높이(h)
  - 또는 **(x_min, y_min, x_max, y_max)**: 좌상단과 우하단 좌표
- 탐지 모델의 출력: 각 물체에 대해 [클래스 라벨, 신뢰도(confidence), bounding box 좌표]
- 한계: 직사각형이므로 물체의 정확한 형태를 표현하지 못한다. 더 정밀한 표현이 필요하면 **segmentation**을 사용한다.

### 3. IoU (Intersection over Union)

탐지 결과의 정확도를 측정하는 핵심 지표다.

- **정의**: 예측 박스와 정답 박스가 겹치는 영역(교집합)을 전체 영역(합집합)으로 나눈 값
- IoU = (두 박스의 겹치는 면적) / (두 박스의 합친 면적)
- 범위: 0 (전혀 겹치지 않음) ~ 1 (완벽히 일치)
- 일반적으로 IoU >= 0.5이면 "올바른 탐지"로 간주한다 (엄격한 기준에서는 0.75).
- 용도:
  - 모델 성능 평가 (mAP 계산 시 기준)
  - Non-Maximum Suppression에서 중복 박스 제거 기준
  - 학습 시 positive/negative 샘플 구분 기준

### 4. YOLO 개념 (You Only Look Once)

객체 탐지의 패러다임을 바꾼 접근법이다.

- **기존 방식 (Two-stage)**: 먼저 물체가 있을 법한 영역을 제안(region proposal)하고, 각 영역을 개별 분류 → 느리다.
- **YOLO (One-stage)**: 이미지를 **한 번만 보고(you only look once)** 모든 물체의 클래스와 위치를 동시에 예측한다.
- YOLO의 핵심 아이디어:
  - 이미지를 S x S 그리드로 나눈다.
  - 각 그리드 셀이 B개의 bounding box와 클래스 확률을 동시에 예측한다.
  - 전체 탐지가 **단일 네트워크의 단일 forward pass**로 완료된다.
- 장점: 매우 빠르다 (실시간 탐지 가능). 이미지 전체의 맥락을 활용한다.
- 단점: 작은 물체나 밀집된 물체에 약할 수 있다.
- 로봇 응용에서 실시간성은 매우 중요하므로, YOLO 계열이 로봇 비전에 널리 사용된다.

### 5. FPN (Feature Pyramid Network)

다양한 크기의 물체를 탐지하기 위한 구조다.

- **문제**: CNN의 깊은 층은 receptive field가 넓어 큰 물체를 잘 감지하지만, 작은 물체의 세부 정보는 잃어버린다. 얕은 층은 세부 정보는 있지만 의미적 이해가 부족하다.
- **FPN의 해결책**: 여러 해상도의 feature map을 **피라미드 구조**로 결합한다.
  - **Bottom-up pathway**: 일반적인 CNN forward pass. 층이 깊어질수록 해상도 감소, 의미 정보 증가.
  - **Top-down pathway**: 깊은 층의 의미 정보를 얕은 층의 해상도로 **업샘플링**하여 전달.
  - **Lateral connection**: 각 수준에서 bottom-up과 top-down 정보를 결합.
- 결과: 모든 스케일에서 풍부한 의미 정보를 가진 feature map을 얻는다.
- 큰 물체는 저해상도 맵에서, 작은 물체는 고해상도 맵에서 탐지한다.
- 현대 탐지 모델(YOLO v3 이후, Faster R-CNN 등)의 표준 구성 요소다.

### 6. VLA 연결: 로봇이 물체를 인식하는 방법

객체 탐지는 VLA의 시각적 이해 능력과 직결된다.

- 로봇이 "빨간 컵을 집어"라는 명령을 수행하려면:
  - 빨간 컵이 **어디에** 있는지 알아야 한다 (detection)
  - 다른 물체와 **구별**해야 한다 (classification)
  - 컵의 **크기와 방향**을 파악해야 한다 (bounding box + 추가 정보)
- VLA 모델은 명시적으로 bounding box를 출력하지 않을 수도 있지만, 내부적으로 vision encoder가 물체의 위치와 정체를 파악하는 것은 동일한 원리다.
- 일부 VLA 파이프라인은 별도의 탐지 모듈을 전처리로 사용하여 "관심 물체"를 먼저 찾고, 그 정보를 행동 결정에 활용한다.

---

## 연습 주제

1. IoU를 직접 계산해 보라: 박스 A = (10, 10, 50, 50), 박스 B = (30, 30, 70, 70) (좌상단-우하단 형식).
2. YOLO에서 이미지를 7x7 그리드로 나누고 셀당 2개의 박스를 예측한다면, 총 몇 개의 후보 박스가 생기는지 계산해 보라.
3. FPN에서 top-down pathway의 업샘플링이 왜 필요한지, 어떤 정보를 전달하는지 설명해 보라.
4. Two-stage detector(예: Faster R-CNN)와 one-stage detector(예: YOLO)의 속도-정확도 트레이드오프를 비교해 보라.
5. 로봇 환경에서 실시간 객체 탐지가 필요한 구체적인 시나리오 세 가지를 생각해 보라.

---
