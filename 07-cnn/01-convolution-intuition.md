# 합성곱 직관 (Convolution Intuition)

## 왜 필요한가

VLA 모델은 카메라 이미지를 입력으로 받아 로봇 행동을 출력한다. 이미지에서 의미 있는 정보를 추출하는 첫 번째 단계가 바로 **합성곱(convolution)** 이다. 합성곱은 "이미지의 어디에 어떤 패턴이 있는가"를 자동으로 찾아내는 연산이며, VLA의 시각 인코더(vision encoder) 내부에서 수만 번 반복된다. 이 노트에서는 합성곱이 무엇인지, 왜 이미지 처리에 적합한지를 직관적으로 이해한다.

---

## 핵심 개념

### 1. Sliding Window와 Kernel

합성곱의 기본 아이디어는 **작은 창(window)을 이미지 위에서 밀어가며(sliding) 패턴을 감지**하는 것이다.

- **Kernel**(또는 filter): 작은 행렬(예: 3x3). 학습 가능한 가중치로 구성된다.
- 커널을 이미지의 한 영역에 올려놓고, 원소별 곱(element-wise multiplication)을 한 뒤 모두 더한다.
- 이 과정을 이미지 전체에 걸쳐 반복하면 하나의 **출력 맵**이 생긴다.

직관: 돋보기로 이미지를 한 칸씩 옮기며 관찰하는 것과 같다. 돋보기의 렌즈(커널)가 특정 패턴에 반응하도록 학습된다.

### 2. Edge Detection 예시

합성곱이 실제로 무엇을 하는지 가장 쉽게 이해하는 방법은 **엣지 검출(edge detection)** 이다.

- 수직 엣지 커널 예시:

```
[-1  0  1]
[-1  0  1]
[-1  0  1]
```

- 이 커널이 이미지 위를 슬라이딩하면, 밝기가 급변하는 수직 경계선 위치에서 높은 값을 출력한다.
- 학습 전에는 이런 커널을 수작업으로 설계했지만, CNN은 데이터로부터 **자동으로** 최적의 커널을 학습한다.

### 3. Stride

**Stride**는 커널이 한 번에 몇 칸씩 이동하는지를 결정한다.

- Stride = 1: 한 픽셀씩 이동. 출력 크기가 입력과 비슷하다.
- Stride = 2: 두 픽셀씩 이동. 출력 크기가 대략 절반으로 줄어든다.
- Stride가 클수록 연산량이 줄고 출력이 작아진다. 일종의 다운샘플링(downsampling) 효과가 있다.

### 4. Padding

커널이 이미지 가장자리에 도달하면 슬라이딩할 공간이 부족하다. 이 문제를 해결하는 것이 **패딩(padding)** 이다.

- **Valid padding**: 패딩 없음. 출력 크기가 입력보다 작아진다.
- **Same padding**: 입력과 출력 크기가 같도록 가장자리에 0을 채운다(zero padding).
- 패딩은 가장자리 정보 손실을 방지하고, 출력 크기를 제어하는 데 사용된다.

### 5. Multiple Channels

실제 이미지는 단일 채널이 아니다.

- 컬러 이미지: 3채널(R, G, B). 커널도 3채널이어야 한다(예: 3x3x3).
- 하나의 커널이 모든 채널에 걸쳐 동시에 연산하고, 결과를 합산하여 **하나의 출력 채널**을 만든다.
- 커널을 여러 개 사용하면 → 여러 개의 출력 채널이 생긴다.
- 입력 채널 수 = 커널의 깊이, 커널 개수 = 출력 채널 수.

### 6. 1x1 Convolution

커널 크기가 1x1인 특수한 합성곱이다.

- 공간적 패턴을 보지 않고, **채널 간의 관계만** 학습한다.
- 채널 수를 늘리거나 줄이는 데 효율적으로 사용된다(차원 축소/확장).
- 연산 비용이 매우 낮으면서도 네트워크의 표현력을 높인다.
- ResNet, Inception 등 현대 아키텍처에서 빈번하게 등장한다.

### 7. Feature Map

합성곱 연산의 출력을 **특징 맵(feature map)** 이라 부른다.

- 초기 층(layer): 엣지, 코너, 색상 변화 같은 **저수준 특징(low-level features)** 을 포착한다.
- 중간 층: 텍스처, 부분적 형태 같은 **중수준 특징**을 포착한다.
- 깊은 층: 물체의 전체적 형태, 의미 있는 구조 같은 **고수준 특징(high-level features)** 을 포착한다.
- VLA 모델의 vision encoder는 이 계층적 특징 추출을 통해 이미지를 이해한다.

### 8. Receptive Field

하나의 출력 뉴런이 "보고 있는" 입력 영역의 크기를 **수용 영역(receptive field)** 이라 한다.

- 층이 깊어질수록 receptive field가 넓어진다.
- 3x3 커널 두 층을 쌓으면 5x5 영역을 본다. 세 층이면 7x7이다.
- 작은 커널을 여러 층 쌓는 것이 큰 커널 하나보다 효율적이면서도 넓은 영역을 볼 수 있다.
- VLA가 이미지의 전체적 맥락을 파악하려면 충분히 넓은 receptive field가 필요하다.

### 9. Translation Equivariance

합성곱의 가장 중요한 성질 중 하나다.

- **이동 등변성(translation equivariance)**: 입력이 이동하면 출력도 같은 만큼 이동한다.
- 물체가 이미지의 왼쪽에 있든 오른쪽에 있든, 같은 커널이 같은 패턴을 감지한다.
- 이 성질 덕분에 CNN은 **위치에 관계없이** 물체를 인식할 수 있다.
- 로봇 카메라 앞에서 물체가 어디에 놓여 있든 인식할 수 있는 이유가 바로 이것이다.

---

## 연습 주제

1. 3x3 커널, stride 1, padding 0일 때 5x5 입력의 출력 크기를 손으로 계산해 보라.
2. Same padding에 필요한 패딩 크기를 커널 크기로부터 유도해 보라.
3. 입력이 32x32x3이고 5x5 커널 10개를 stride 1, same padding으로 적용하면 출력 형태(shape)와 파라미터 수는?
4. 3x3 커널 3층의 receptive field와 7x7 커널 1층의 receptive field를 비교하고, 파라미터 수 차이를 계산해 보라.
5. 1x1 convolution이 채널 수를 256에서 64로 줄일 때 파라미터 수를 계산해 보라.

---
