# 행렬 연산 (Matrix Operations)

## 왜 필요한가?
신경망의 순전파(forward pass)는 본질적으로 행렬 곱셈의 연속이다. 입력 벡터에 가중치 행렬을 곱하는 것이 곧 신경망의 한 층(layer)을 통과하는 것이다.

---

## 1. 내적 (Dot Product)
두 벡터의 대응 원소를 곱해서 모두 더한 값. 결과는 **스칼라**.

$$a = [1, 2, 3], \quad b = [4, 5, 6]$$

$$a \cdot b = 1 \times 4 + 2 \times 5 + 3 \times 6 = 4 + 10 + 18 = 32$$

### 내적의 의미
- **유사도**: 두 벡터가 같은 방향이면 내적이 크고, 직교하면 0, 반대 방향이면 음수
- 딥러닝에서 **어텐션(attention) 점수**가 바로 Query와 Key 벡터의 내적이다
- 코사인 유사도: $\cos(\theta) = \dfrac{a \cdot b}{\|a\| \times \|b\|}$

### 규칙
- 두 벡터의 **차원이 같아야** 한다
- 결과는 항상 스칼라 (숫자 하나)

## 2. 행렬 곱셈 (Matrix Multiplication)
행렬 A의 각 **행**과 행렬 B의 각 **열**의 내적을 계산한다.

$A$ $(2 \times 3)$, $B$ $(3 \times 2)$ → $C = A \times B$ $(2 \times 2)$

```
[1  2  3]     [7  10]        [1×7+2×8+3×9    1×10+2×11+3×12]   [50   68]
[4  5  6]  ×  [8  11]   =    [4×7+5×8+6×9    4×10+5×11+6×12] = [122  167]
              [9  12]
```

### 핵심 규칙
- A의 **열 수** = B의 **행 수** 이어야 곱셈 가능
- $(m \times n) \times (n \times p) = (m \times p)$
- **교환 법칙 성립 안 함**: $A \times B \neq B \times A$ (일반적으로)

### 신경망에서의 행렬 곱셈

```
입력:    x  (1 × 입력차원)     예: (1 × 784)   — MNIST 이미지 1장을 펼친 것
가중치:  W  (입력차원 × 출력차원)  예: (784 × 256)
편향:    b  (1 × 출력차원)       예: (1 × 256)
```

$$y = xW + b$$

→ 하나의 Linear Layer = 행렬 곱셈 + 편향 덧셈

### 배치 처리
실제로는 여러 입력을 한번에 처리한다 (배치).

```
입력:    X  (배치 × 입력차원)    예: (32 × 784)   — 32장의 이미지
가중치:  W  (입력차원 × 출력차원)  예: (784 × 256)
```

$$Y = XW + b$$

→ 배치 크기에 관계없이 같은 가중치 $W$를 사용. 이것이 행렬 곱셈의 효율성.

## 3. 원소별 연산 (Element-wise Operations)
같은 shape의 두 텐서에서 같은 위치끼리 연산.

```
[1, 2, 3] + [4, 5, 6] = [5, 7, 9]      # 원소별 덧셈
[1, 2, 3] * [4, 5, 6] = [4, 10, 18]     # 원소별 곱셈 (Hadamard product)
```

딥러닝에서의 예:
- 활성화 함수 적용: 모든 원소에 ReLU를 적용
- 마스킹: 특정 위치를 0으로 만들기 (dropout, attention mask)

## 4. 행렬 전치 (Transpose) 복습
$(m \times n) \to (n \times m)$. 행과 열을 뒤집는다.

중요한 성질: $(AB)^T = B^T A^T$ — 곱셈의 전치는 순서가 뒤집힌다.

어텐션에서: $\text{Attention} = \text{softmax}\!\left(\dfrac{QK^T}{\sqrt{d}}\right)$ — $K$를 전치해서 $Q$와 내적한다.

## 5. 행렬의 역 (Inverse) — 개념만
정방 행렬 $A$에 대해, $A \times A^{-1} = I$ (단위행렬)를 만족하는 $A^{-1}$.

딥러닝에서 직접 역행렬을 계산할 일은 거의 없지만, 최적화 이론(뉴턴 방법 등)에서 개념적으로 등장한다.

---

## 핵심 정리
1. **내적** = 두 벡터의 유사도. 어텐션의 핵심 연산
2. **행렬 곱셈** = 신경망 한 층의 순전파. $(m \times n) \times (n \times p) = (m \times p)$
3. **원소별 연산** = 활성화 함수, 마스킹 등에 사용
4. Shape 호환성 확인이 가장 중요: "안쪽 차원이 같아야 곱셈 가능"

---

## 실습 과제
1. $2 \times 3$ 행렬과 $3 \times 2$ 행렬의 곱을 손으로 계산해보기
2. 2층 신경망의 순전파를 행렬 곱셈으로 표현해보기: 입력$(1 \times 4)$ → 은닉$(1 \times 3)$ → 출력$(1 \times 2)$
3. 내적이 유사도인 이유를 직관적으로 설명해보기 (같은 방향 벡터 vs 직교 벡터)

---
