# 벡터와 행렬 (Vectors and Matrices)

## 왜 필요한가?
신경망의 모든 데이터(이미지, 텍스트, 로봇 동작)는 숫자 배열로 표현된다. 벡터와 행렬은 이 숫자 배열을 다루는 기본 단위다. VLA 모델에서 이미지는 3차원 텐서, 텍스트는 토큰 벡터의 시퀀스, 로봇 동작은 7차원 벡터로 표현된다.

---

## 1. 스칼라 (Scalar)
하나의 숫자. 차원이 없다.

```
x = 3.14
temperature = 36.5
```

딥러닝에서의 예: 학습률(learning rate), 손실값(loss)

## 2. 벡터 (Vector)
숫자를 일렬로 나열한 것. **1차원 배열**.

```
v = [1, 2, 3]          # 3차원 벡터
```

딥러닝에서의 예:
- 단어 임베딩: 하나의 단어를 표현하는 벡터 (예: 512차원)
- Fairino FR3의 관절 각도: 6개의 숫자 → 6차원 벡터
- VLA 액션: `[dx, dy, dz, droll, dpitch, dyaw, gripper]` → 7차원 벡터 (6-DoF 포즈 + 그리퍼)

### 벡터의 기본 연산
- **덧셈**: 같은 위치끼리 더한다. $[1,2] + [3,4] = [4,6]$
- **스칼라 곱**: 모든 원소에 같은 수를 곱한다. $2 \times [1,2,3] = [2,4,6]$
- **크기(norm)**: 벡터의 "길이". $\|v\| = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2}$

## 3. 행렬 (Matrix)
숫자를 직사각형으로 배열한 것. **2차원 배열**. 행(row)과 열(column)로 구성.

```
M = [[1, 2, 3],
     [4, 5, 6]]        # 2x3 행렬 (2행 3열)
```

**행렬의 크기(shape)**: $(m, n)$ = (행의 수, 열의 수)

딥러닝에서의 예:
- 신경망의 가중치(weight): 입력 차원 × 출력 차원 크기의 행렬
- 흑백 이미지: 높이 × 너비 크기의 행렬 (예: 28×28 MNIST)

### 전치 (Transpose)
행과 열을 뒤집는다. $M$의 전치는 $M^T$.

```
M = [[1, 2, 3],       M^T = [[1, 4],
     [4, 5, 6]]              [2, 5],
                              [3, 6]]
(2×3)                  (3×2)
```

## 4. 텐서 (Tensor)
벡터(1차원), 행렬(2차원)을 일반화한 **다차원 배열**.

| 차원 | 이름 | 예시 |
|------|------|------|
| 0 | 스칼라 | loss = 0.5 |
| 1 | 벡터 | 로봇 관절 각도 [7] |
| 2 | 행렬 | 흑백 이미지 [28, 28] |
| 3 | 3D 텐서 | 컬러 이미지 [3, 224, 224] (채널, 높이, 너비) |
| 4 | 4D 텐서 | 이미지 배치 [32, 3, 224, 224] (배치, 채널, 높이, 너비) |

### Shape 읽는 법
텐서의 shape은 각 차원의 크기를 나타낸다.

```
이미지 배치: shape = (32, 3, 224, 224)
→ 32장의 이미지, 각 이미지는 3채널(RGB), 224x224 픽셀
```

VLA 모델에서:
- 이미지 입력: `(batch, 3, 224, 224)`
- 텍스트 입력: `(batch, sequence_length)` — 각 값은 토큰 ID
- 로봇 액션 출력: `(batch, 7)` — 7-DoF 액션

## 5. 인덱싱 (Indexing)
텐서의 특정 원소나 부분을 꺼내는 것. **0부터 시작**한다.

```
v = [10, 20, 30, 40]
v[0] = 10              # 첫 번째 원소
v[-1] = 40             # 마지막 원소
v[1:3] = [20, 30]      # 슬라이싱: 인덱스 1부터 2까지
```

행렬에서:
```
M = [[1, 2, 3],
     [4, 5, 6]]

M[0, 2] = 3            # 0번 행, 2번 열
M[1, :] = [4, 5, 6]    # 1번 행 전체
M[:, 0] = [1, 4]       # 0번 열 전체
```

## 6. Reshape
텐서의 shape을 바꾸는 연산. 원소의 총 수는 변하지 않는다.

```
원본: shape (2, 3) → 원소 6개
[[1, 2, 3],
 [4, 5, 6]]

reshape(3, 2):
[[1, 2],
 [3, 4],
 [5, 6]]

reshape(6,):
[1, 2, 3, 4, 5, 6]     # 1차원으로 펼치기 (flatten)
```

VLA 모델에서 이미지를 패치로 나눌 때, ViT는 $(3, 224, 224)$ 이미지를 $(196, 768)$ 형태로 reshape한다 (196개 패치, 각 768차원).

---

## 핵심 정리
1. **스칼라 → 벡터 → 행렬 → 텐서**는 차원이 늘어나는 순서
2. 딥러닝의 모든 데이터는 텐서로 표현된다
3. **Shape**을 읽는 능력이 가장 중요하다 — 디버깅의 80%가 shape 문제
4. 신경망의 가중치는 행렬이고, 순전파(forward pass)는 행렬 연산이다

---

## 실습 과제
1. 3×3 흑백 이미지를 행렬로 직접 표현해보기
2. RGB 컬러 이미지(3×3 크기)를 3차원 텐서로 표현해보기
3. $(2, 3, 4)$ shape의 텐서에서 총 원소 수를 계산하고, 가능한 reshape 형태를 3가지 이상 나열해보기
4. Fairino FR3의 6-DoF 관절 벡터 `[0.1, -0.05, 0.2, 0.0, 0.1, -0.1]`에서 각 원소가 무엇을 의미하는지 조사해보기

---

## 다음 노트
→ [행렬 연산](./02-matrix-operations.md): 내적, 행렬 곱셈 — 신경망 순전파의 핵심 연산
