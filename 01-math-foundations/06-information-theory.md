# 정보이론 (Information Theory)

## 왜 필요한가?
딥러닝에서 가장 많이 사용하는 손실 함수인 **크로스엔트로피(cross-entropy)**는 정보이론에서 온 개념이다. 모델의 예측 분포와 실제 분포의 차이를 측정하는 수학적 도구.

---

## 1. 정보량 (Information Content)
놀라운 사건일수록 정보량이 크다.

$$I(x) = -\log_2 P(x)$$

| $P(x)$ | 정보량 | 의미 |
|---------|--------|------|
| $1$ (확실한 사건) | $0$ | 놀랍지 않음 |
| $0.5$ | $1$ bit | 보통 |
| $0.01$ | $\approx 6.6$ bit | 매우 놀라움 |

직관: "해가 동쪽에서 뜬다"(확실) vs "복권 1등 당첨"(극히 낮은 확률) — 후자가 더 많은 정보를 담고 있다.

## 2. 엔트로피 (Entropy)
확률 분포의 **평균 정보량**. 불확실성의 척도.

$$H(P) = -\sum_x P(x) \log P(x)$$

### 예시

| 분포 | 엔트로피 |
|------|----------|
| 공정한 동전 | $H = -(0.5 \log_2 0.5 + 0.5 \log_2 0.5) = 1$ bit |
| 편향된 동전 | $H = -(0.9 \log_2 0.9 + 0.1 \log_2 0.1) \approx 0.47$ bit |
| 확실한 결과 | $H = -(1.0 \log_2 1.0) = 0$ bit |

→ **불확실할수록 엔트로피가 높다**. 균일 분포가 최대 엔트로피.

## 3. 크로스엔트로피 (Cross-Entropy)
**실제 분포 $P$**와 **예측 분포 $Q$** 사이의 차이를 측정.

$$H(P, Q) = -\sum_x P(x) \log Q(x)$$

### 분류에서의 크로스엔트로피
실제 레이블이 원-핫 벡터일 때 (하나만 1, 나머지 0):

```
실제: P = [0, 0, 1]  (정답: 클래스 3)
예측: Q = [0.1, 0.2, 0.7]
```

$$H(P, Q) = -(0 \cdot \log 0.1 + 0 \cdot \log 0.2 + 1 \cdot \log 0.7) = -\log(0.7) \approx 0.357$$

→ 결국 **정답 클래스의 예측 확률에 $-\log$를 취한 것**!

좋은 예측($Q$가 $P$에 가까울수록): 크로스엔트로피 ↓
나쁜 예측($Q$가 $P$에서 멀수록): 크로스엔트로피 ↑

| 정답 확신도 | 손실 |
|------------|------|
| $-\log(0.99) \approx 0.01$ | 손실 낮음 |
| $-\log(0.01) \approx 4.61$ | 손실 높음 |

## 4. KL 다이버전스 (KL Divergence)
두 확률 분포 사이의 "거리" (정확히는 비대칭적).

$$D_{KL}(P \| Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)} = H(P, Q) - H(P)$$

→ 크로스엔트로피 = 엔트로피 + KL 다이버전스

엔트로피 $H(P)$는 고정값이므로, **크로스엔트로피를 최소화하는 것 = KL 다이버전스를 최소화하는 것** = 예측 분포를 실제 분포에 가깝게 만드는 것.

딥러닝에서의 사용:
- VAE: 잠재 분포와 사전 분포 사이의 KL 다이버전스 최소화
- Knowledge Distillation: 교사 모델과 학생 모델 분포의 KL 다이버전스
- RLHF: 정책 모델이 기본 모델에서 너무 멀어지지 않도록 KL 페널티

## 5. 크로스엔트로피와 MLE의 관계
이전 노트에서 배운 MLE와의 연결:

$$\begin{aligned}
&\text{크로스엔트로피 최소화} \\
&= -\sum_x P(x) \log Q(x) \text{ 최소화} \\
&= -\sum_i \log Q(\text{정답}_i) \text{ 최소화} \quad \text{(원-핫 레이블이면)} \\
&= \text{로그 우도 최대화 (MLE)}
\end{aligned}$$

→ **딥러닝 학습의 세 가지 동일한 표현**:
1. 크로스엔트로피 손실 최소화
2. KL 다이버전스 최소화
3. 최대 우도 추정 (MLE)

---

## 핵심 정리
1. **엔트로피**: 불확실성의 척도. 균일분포가 최대
2. **크로스엔트로피**: 실제 vs 예측 분포의 차이. 가장 흔한 분류 손실 함수
3. **KL 다이버전스**: 두 분포의 비대칭적 거리. VAE, RLHF 등에서 사용
4. 크로스엔트로피 최소화 = KL 다이버전스 최소화 = MLE

---

## 실습 과제
1. 세 개의 분포에 대해 엔트로피를 계산하고 비교해보기: $[1/3, 1/3, 1/3]$, $[0.8, 0.1, 0.1]$, $[1, 0, 0]$
2. 정답이 클래스 2(인덱스 1)일 때, 예측 $[0.1, 0.7, 0.2]$와 $[0.1, 0.2, 0.7]$의 크로스엔트로피를 비교해보기
3. 왜 크로스엔트로피가 MSE보다 분류 문제에 더 적합한지 직관적으로 설명해보기

---

## 수학 기초 완료!
수학 기초 파트를 모두 마쳤습니다. 다음으로 넘어가세요.

→ 다음 파트: `02-python-fundamentals/` — 파이썬 기초
